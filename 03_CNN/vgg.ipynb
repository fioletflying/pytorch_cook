{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import optim\n",
    "import time\n",
    "import util_fei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform= transforms.Compose([transforms.Resize(224),\n",
    "                                transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, \n",
    "                                kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    # 这里的* 代表对layer里面的元素进行解压成单个的元素\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch):\n",
    "    conv_blks = []\n",
    "    in_channels = 1\n",
    "    for (num_convs, out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "    return nn.Sequential(\n",
    "        *conv_blks,\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(out_channels * 7 * 7, 4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\n",
    "model = vgg(conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 112, 112])\n",
      "Sequential output shape:\t torch.Size([1, 128, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 256, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 512, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "Flatten output shape:\t torch.Size([1, 25088])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(size=(1, 1, 224, 224))\n",
    "for blk in model:\n",
    "    X = blk(X)\n",
    "    print(blk.__class__.__name__,'output shape:\\t',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 4\n",
    "small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]\n",
    "net = vgg(small_conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 16, 112, 112])\n",
      "Sequential output shape:\t torch.Size([1, 32, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 64, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 128, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 128, 7, 7])\n",
      "Flatten output shape:\t torch.Size([1, 6272])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(size=(1, 1, 224, 224))\n",
    "for blk in net:\n",
    "    X = blk(X)\n",
    "    print(blk.__class__.__name__,'output shape:\\t',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/study/git/pytorch_cook/03_CNN/util_fei.py:26: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(m.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda:0\n",
      "epoch: 1,\t                train_loss: 1.147861,\t                train_correct:0.577017,\t                test_correct:0.828100,\t                test time:76.716375\n",
      "epoch: 2,\t                train_loss: 0.415131,\t                train_correct:0.846883,\t                test_correct:0.872700,\t                test time:76.080905\n",
      "epoch: 3,\t                train_loss: 0.333832,\t                train_correct:0.877617,\t                test_correct:0.886200,\t                test time:77.268801\n",
      "epoch: 4,\t                train_loss: 0.291390,\t                train_correct:0.892500,\t                test_correct:0.896500,\t                test time:77.540567\n",
      "epoch: 5,\t                train_loss: 0.263489,\t                train_correct:0.902633,\t                test_correct:0.903600,\t                test time:75.713849\n",
      "epoch: 6,\t                train_loss: 0.239466,\t                train_correct:0.911483,\t                test_correct:0.908600,\t                test time:76.361871\n",
      "epoch: 7,\t                train_loss: 0.220795,\t                train_correct:0.918233,\t                test_correct:0.913300,\t                test time:78.206968\n",
      "epoch: 8,\t                train_loss: 0.202051,\t                train_correct:0.924417,\t                test_correct:0.915300,\t                test time:75.959217\n",
      "epoch: 9,\t                train_loss: 0.187988,\t                train_correct:0.930300,\t                test_correct:0.919500,\t                test time:76.122470\n",
      "epoch: 10,\t                train_loss: 0.170118,\t                train_correct:0.937000,\t                test_correct:0.924400,\t                test time:74.994520\n"
     ]
    }
   ],
   "source": [
    "creterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.05)\n",
    "\n",
    "epoch_nums = 10\n",
    "\n",
    "util_fei.train_model(net, train_dataloader, test_dataloader,creterion, optimizer,device, epoch_num=epoch_nums)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
