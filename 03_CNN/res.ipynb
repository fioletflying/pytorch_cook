{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "怎么理解res net 的作用？\n",
    "用通俗的理解就是，由于res net 添加了一个short cut, 这个捷径加入后，就是说及时没有添加这个新层的也可以保证原来训练的数据函数可以继续使用。另外一个层面上，也可以理解，在新加的层面上如果有新的模型添加，再加上原来训练的函数，这样新学习的东西不会因为新层的增加导致整个学习的函数会被重新学习过，而丢弃远学习的东西。\n",
    "其基本的原则也是：每个附加层都应该更容易地包含原始函数作为其元素之⼀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "import util_fei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                use_1x1conv=False, strides=1):\n",
    "                super().__init__()\n",
    "\n",
    "                self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=1,\n",
    "                                        stride=strides)\n",
    "                self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=1)\n",
    "                if use_1x1conv:\n",
    "                    self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
    "                                            kernel_size=1,\n",
    "                                            stride=strides)\n",
    "                else:\n",
    "                    self.conv3 = None\n",
    "                self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "                self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3 is not None:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "# 高宽不变\n",
    "blk = Residual(3,3)\n",
    "X = torch.rand(4, 3, 6, 6)\n",
    "Y = blk(X)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# 高宽减半\n",
    "blk = Residual(3,6, use_1x1conv=True, strides=2)\n",
    "X = torch.rand(4, 3, 6, 6)\n",
    "Y = blk(X)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(input_channels, num_channels,\n",
    "                num_residuals, first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(input_channels,num_channels,\n",
    "                            use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels, num_channels))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(nn.Conv2d(1,64, kernel_size=7,\n",
    "                    stride=2, padding=3),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
    "b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
    "b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
    "b5 = nn.Sequential(*resnet_block(256, 512, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(b1, b2, b3, b4, b5,\n",
    "                    nn.AdaptiveAvgPool2d((1,1)),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(512,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 128, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 256, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 512])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 224, 224))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t', X.shape)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "117e442dca80433ebd14c647c5435de0b3bd3a721296d97cd0475c1b541bf910"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
