{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun  8 09:05:15 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.94       Driver Version: 470.94       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 30%   38C    P0   103W / 350W |      0MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:82:00.0 Off |                  N/A |\n",
      "| 30%   39C    P0   101W / 350W |      0MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 22:43:04.588203: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-02 22:43:05.847185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22302 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "2022-06-02 22:43:05.848995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22302 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:82:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 224, 224, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "url='n02099601_3004.jpg'\n",
    "img = resize(io.imread(url), (224, 224))\n",
    "input_batch = 255*np.array(np.repeat(np.expand_dims(np.array(img, dtype=np.float32), axis=0), BATCH_SIZE, axis=0), dtype=np.float32)\n",
    "\n",
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 22:43:13.998373: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-06-02 22:43:15.704411: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201\n",
      "2022-06-02 22:43:16.886902: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class | Probability (out of 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 22:43:18.268654: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(160, 0.32256573),\n",
       " (169, 0.23734361),\n",
       " (212, 0.18466648),\n",
       " (170, 0.08061244),\n",
       " (207, 0.030543853)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(input_batch) # warm up\n",
    "indices = (-predictions[0]).argsort()[:5]\n",
    "print(\"Class | Probability (out of 1)\")\n",
    "list(zip(indices, predictions[0][indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.1 ms ± 715 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "result = model.predict_on_batch(input_batch) # Check default performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/tf27/lib/python3.9/site-packages/tensorflow/python/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/tf27/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-06-02 22:51:28,609 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-06-02 22:51:35,434 - INFO - Signatures found in model: [serving_default].\n",
      "2022-06-02 22:51:35,434 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-06-02 22:51:35,436 - INFO - Output names: ['predictions']\n",
      "WARNING:tensorflow:From /root/miniconda3/envs/tf27/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-06-02 22:51:38,970 - WARNING - From /root/miniconda3/envs/tf27/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-06-02 22:51:40,723 - INFO - Using tensorflow=2.6.0, onnx=1.10.1, tf2onnx=1.9.3/55c08b\n",
      "2022-06-02 22:51:40,723 - INFO - Using opset <onnx, 9>\n",
      "2022-06-02 22:51:49,415 - INFO - Computed 0 values for constant folding\n",
      "2022-06-02 22:51:55,087 - INFO - Optimizing ONNX model\n",
      "2022-06-02 22:51:59,510 - INFO - After optimization: Add -1 (18->17), BatchNormalization -53 (53->0), Const -162 (270->108), GlobalAveragePool +1 (0->1), Identity -57 (57->0), ReduceMean -1 (1->0), Squeeze +1 (0->1), Transpose -213 (214->1)\n",
      "2022-06-02 22:52:01,050 - INFO - \n",
      "2022-06-02 22:52:01,050 - INFO - Successfully converted TensorFlow model my_model to ONNX\n",
      "2022-06-02 22:52:01,050 - INFO - Model inputs: ['input_1']\n",
      "2022-06-02 22:52:01,050 - INFO - Model outputs: ['predictions']\n",
      "2022-06-02 22:52:01,050 - INFO - ONNX model is saved at temp.onnx\n"
     ]
    }
   ],
   "source": [
    "model.save('my_model')\n",
    "!python -m tf2onnx.convert --saved-model my_model --output temp.onnx\n",
    "onnx_model = onnx.load_model('temp.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = onnx_model.graph.input\n",
    "for input in inputs:\n",
    "    dim1 = input.type.tensor_type.shape.dim[0]\n",
    "    dim1.dim_value = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done saving!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"resnet50_onnx_model_tf.onnx\"\n",
    "onnx.save_model(onnx_model, model_name)\n",
    "print(\"Done saving!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restarting kernel  in three seconds...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "print(\"Restarting kernel  in three seconds...\")\n",
    "time.sleep(3)\n",
    "print(\"Restarting kernel now\")\n",
    "os._exit(0) # Shut down all kernels so TRT doesn't fight with Tensorflow fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "USE_FP16 = True\n",
    "\n",
    "target_dtype = np.float16 if USE_FP16 else np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 224, 224, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "url='n02099601_3004.jpg'\n",
    "img = resize(io.imread(url), (224, 224))\n",
    "input_batch = 255*np.array(np.repeat(np.expand_dims(np.array(img, dtype=np.float32), axis=0), BATCH_SIZE, axis=0), dtype=np.float32)\n",
    "\n",
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = input_batch.astype(target_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8204] # trtexec --onnx=resnet50_onnx_model_tf.onnx --saveEngine=resnet_engine.trt --explicitBatch --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw --fp16\n",
      "[06/02/2022-22:59:06] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[06/02/2022-22:59:06] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[06/02/2022-22:59:06] [I] === Model Options ===\n",
      "[06/02/2022-22:59:06] [I] Format: ONNX\n",
      "[06/02/2022-22:59:06] [I] Model: resnet50_onnx_model_tf.onnx\n",
      "[06/02/2022-22:59:06] [I] Output:\n",
      "[06/02/2022-22:59:06] [I] === Build Options ===\n",
      "[06/02/2022-22:59:06] [I] Max batch: explicit batch\n",
      "[06/02/2022-22:59:06] [I] Workspace: 16 MiB\n",
      "[06/02/2022-22:59:06] [I] minTiming: 1\n",
      "[06/02/2022-22:59:06] [I] avgTiming: 8\n",
      "[06/02/2022-22:59:06] [I] Precision: FP32+FP16\n",
      "[06/02/2022-22:59:06] [I] Calibration: \n",
      "[06/02/2022-22:59:06] [I] Refit: Disabled\n",
      "[06/02/2022-22:59:06] [I] Sparsity: Disabled\n",
      "[06/02/2022-22:59:06] [I] Safe mode: Disabled\n",
      "[06/02/2022-22:59:06] [I] DirectIO mode: Disabled\n",
      "[06/02/2022-22:59:06] [I] Restricted mode: Disabled\n",
      "[06/02/2022-22:59:06] [I] Save engine: resnet_engine.trt\n",
      "[06/02/2022-22:59:06] [I] Load engine: \n",
      "[06/02/2022-22:59:06] [I] Profiling verbosity: 0\n",
      "[06/02/2022-22:59:06] [I] Tactic sources: Using default tactic sources\n",
      "[06/02/2022-22:59:06] [I] timingCacheMode: local\n",
      "[06/02/2022-22:59:06] [I] timingCacheFile: \n",
      "[06/02/2022-22:59:06] [I] Input(s): fp16:chw\n",
      "[06/02/2022-22:59:06] [I] Output(s): fp16:chw\n",
      "[06/02/2022-22:59:06] [I] Input build shapes: model\n",
      "[06/02/2022-22:59:06] [I] Input calibration shapes: model\n",
      "[06/02/2022-22:59:06] [I] === System Options ===\n",
      "[06/02/2022-22:59:06] [I] Device: 0\n",
      "[06/02/2022-22:59:06] [I] DLACore: \n",
      "[06/02/2022-22:59:06] [I] Plugins:\n",
      "[06/02/2022-22:59:06] [I] === Inference Options ===\n",
      "[06/02/2022-22:59:06] [I] Batch: Explicit\n",
      "[06/02/2022-22:59:06] [I] Input inference shapes: model\n",
      "[06/02/2022-22:59:06] [I] Iterations: 10\n",
      "[06/02/2022-22:59:06] [I] Duration: 3s (+ 200ms warm up)\n",
      "[06/02/2022-22:59:06] [I] Sleep time: 0ms\n",
      "[06/02/2022-22:59:06] [I] Idle time: 0ms\n",
      "[06/02/2022-22:59:06] [I] Streams: 1\n",
      "[06/02/2022-22:59:06] [I] ExposeDMA: Disabled\n",
      "[06/02/2022-22:59:06] [I] Data transfers: Enabled\n",
      "[06/02/2022-22:59:06] [I] Spin-wait: Disabled\n",
      "[06/02/2022-22:59:06] [I] Multithreading: Disabled\n",
      "[06/02/2022-22:59:06] [I] CUDA Graph: Disabled\n",
      "[06/02/2022-22:59:06] [I] Separate profiling: Disabled\n",
      "[06/02/2022-22:59:06] [I] Time Deserialize: Disabled\n",
      "[06/02/2022-22:59:06] [I] Time Refit: Disabled\n",
      "[06/02/2022-22:59:06] [I] Skip inference: Disabled\n",
      "[06/02/2022-22:59:06] [I] Inputs:\n",
      "[06/02/2022-22:59:06] [I] === Reporting Options ===\n",
      "[06/02/2022-22:59:06] [I] Verbose: Disabled\n",
      "[06/02/2022-22:59:06] [I] Averages: 10 inferences\n",
      "[06/02/2022-22:59:06] [I] Percentile: 99\n",
      "[06/02/2022-22:59:06] [I] Dump refittable layers:Disabled\n",
      "[06/02/2022-22:59:06] [I] Dump output: Disabled\n",
      "[06/02/2022-22:59:06] [I] Profile: Disabled\n",
      "[06/02/2022-22:59:06] [I] Export timing to JSON file: \n",
      "[06/02/2022-22:59:06] [I] Export output to JSON file: \n",
      "[06/02/2022-22:59:06] [I] Export profile to JSON file: \n",
      "[06/02/2022-22:59:06] [I] \n",
      "[06/02/2022-22:59:06] [I] === Device Information ===\n",
      "[06/02/2022-22:59:06] [I] Selected Device: NVIDIA GeForce RTX 3090\n",
      "[06/02/2022-22:59:06] [I] Compute Capability: 8.6\n",
      "[06/02/2022-22:59:06] [I] SMs: 82\n",
      "[06/02/2022-22:59:06] [I] Compute Clock Rate: 1.695 GHz\n",
      "[06/02/2022-22:59:06] [I] Device Global Memory: 24268 MiB\n",
      "[06/02/2022-22:59:06] [I] Shared Memory per SM: 100 KiB\n",
      "[06/02/2022-22:59:06] [I] Memory Bus Width: 384 bits (ECC disabled)\n",
      "[06/02/2022-22:59:06] [I] Memory Clock Rate: 9.751 GHz\n",
      "[06/02/2022-22:59:06] [I] \n",
      "[06/02/2022-22:59:06] [I] TensorRT version: 8.2.4\n",
      "[06/02/2022-22:59:07] [I] [TRT] [MemUsageChange] Init CUDA: CPU +459, GPU +0, now: CPU 471, GPU 489 (MiB)\n",
      "[06/02/2022-22:59:07] [I] [TRT] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 471 MiB, GPU 489 MiB\n",
      "[06/02/2022-22:59:07] [I] [TRT] [MemUsageSnapshot] End constructing builder kernel library: CPU 625 MiB, GPU 533 MiB\n",
      "[06/02/2022-22:59:07] [I] Start parsing network model\n",
      "[06/02/2022-22:59:07] [I] [TRT] ----------------------------------------------------------------\n",
      "[06/02/2022-22:59:07] [I] [TRT] Input filename:   resnet50_onnx_model_tf.onnx\n",
      "[06/02/2022-22:59:07] [I] [TRT] ONNX IR version:  0.0.4\n",
      "[06/02/2022-22:59:07] [I] [TRT] Opset version:    9\n",
      "[06/02/2022-22:59:07] [I] [TRT] Producer name:    tf2onnx\n",
      "[06/02/2022-22:59:07] [I] [TRT] Producer version: 1.9.3\n",
      "[06/02/2022-22:59:07] [I] [TRT] Domain:           \n",
      "[06/02/2022-22:59:07] [I] [TRT] Model version:    0\n",
      "[06/02/2022-22:59:07] [I] [TRT] Doc string:       \n",
      "[06/02/2022-22:59:07] [I] [TRT] ----------------------------------------------------------------\n",
      "[06/02/2022-22:59:08] [W] [TRT] onnx2trt_utils.cpp:366: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[06/02/2022-22:59:08] [W] [TRT] ShapedWeights.cpp:173: Weights StatefulPartitionedCall/resnet50/predictions/MatMul/ReadVariableOp:0 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[06/02/2022-22:59:08] [I] Finish parsing network model\n",
      "[06/02/2022-22:59:09] [W] [TRT] TensorRT was linked against cuBLAS/cuBLASLt 11.6.5 but loaded cuBLAS/cuBLASLt 11.5.2\n",
      "[06/02/2022-22:59:09] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +751, GPU +318, now: CPU 1480, GPU 851 (MiB)\n",
      "[06/02/2022-22:59:10] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +674, GPU +344, now: CPU 2154, GPU 1195 (MiB)\n",
      "[06/02/2022-22:59:10] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[06/02/2022-22:59:23] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output.\n",
      "[06/02/2022-23:00:18] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[06/02/2022-23:00:18] [I] [TRT] Total Host Persistent Memory: 130912\n",
      "[06/02/2022-23:00:18] [I] [TRT] Total Device Persistent Memory: 51271680\n",
      "[06/02/2022-23:00:18] [I] [TRT] Total Scratch Memory: 0\n",
      "[06/02/2022-23:00:18] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 53 MiB, GPU 263 MiB\n",
      "[06/02/2022-23:00:18] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 2.85496ms to assign 3 blocks to 62 nodes requiring 117456896 bytes.\n",
      "[06/02/2022-23:00:18] [I] [TRT] Total Activation Memory: 117456896\n",
      "[06/02/2022-23:00:18] [W] [TRT] TensorRT was linked against cuBLAS/cuBLASLt 11.6.5 but loaded cuBLAS/cuBLASLt 11.5.2\n",
      "[06/02/2022-23:00:18] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +8, now: CPU 3179, GPU 1725 (MiB)\n",
      "[06/02/2022-23:00:18] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 3179, GPU 1735 (MiB)\n",
      "[06/02/2022-23:00:18] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +49, GPU +49, now: CPU 49, GPU 49 (MiB)\n",
      "[06/02/2022-23:00:18] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 3174, GPU 1651 (MiB)\n",
      "[06/02/2022-23:00:18] [I] [TRT] Loaded engine size: 49 MiB\n",
      "[06/02/2022-23:00:18] [W] [TRT] TensorRT was linked against cuBLAS/cuBLASLt 11.6.5 but loaded cuBLAS/cuBLASLt 11.5.2\n",
      "[06/02/2022-23:00:18] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 3175, GPU 1711 (MiB)\n",
      "[06/02/2022-23:00:18] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 3175, GPU 1719 (MiB)\n",
      "[06/02/2022-23:00:18] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +48, now: CPU 0, GPU 48 (MiB)\n",
      "[06/02/2022-23:00:19] [I] Engine built in 72.5348 sec.\n",
      "[06/02/2022-23:00:19] [W] [TRT] TensorRT was linked against cuBLAS/cuBLASLt 11.6.5 but loaded cuBLAS/cuBLASLt 11.5.2\n",
      "[06/02/2022-23:00:19] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2866, GPU 1683 (MiB)\n",
      "[06/02/2022-23:00:19] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +8, now: CPU 2867, GPU 1691 (MiB)\n",
      "[06/02/2022-23:00:19] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +161, now: CPU 0, GPU 209 (MiB)\n",
      "[06/02/2022-23:00:19] [I] Using random values for input input_1\n",
      "[06/02/2022-23:00:19] [I] Created input binding for input_1 with dimensions 32x224x224x3\n",
      "[06/02/2022-23:00:19] [I] Using random values for output predictions\n",
      "[06/02/2022-23:00:19] [I] Created output binding for predictions with dimensions 32x1000\n",
      "[06/02/2022-23:00:19] [I] Starting inference\n",
      "[06/02/2022-23:00:22] [I] Warmup completed 45 queries over 200 ms\n",
      "[06/02/2022-23:00:22] [I] Timing trace has 684 queries over 3.01284 s\n",
      "[06/02/2022-23:00:22] [I] \n",
      "[06/02/2022-23:00:22] [I] === Trace details ===\n",
      "[06/02/2022-23:00:22] [I] Trace averages of 10 runs:\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.37269 ms - Host latency: 5.20217 ms (end to end 8.55363 ms, enqueue 0.841682 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.36224 ms - Host latency: 5.19362 ms (end to end 8.23068 ms, enqueue 0.82128 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.36705 ms - Host latency: 5.20145 ms (end to end 8.53557 ms, enqueue 0.843961 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.37217 ms - Host latency: 5.20172 ms (end to end 8.29501 ms, enqueue 0.851154 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39163 ms - Host latency: 5.21773 ms (end to end 8.59818 ms, enqueue 0.836816 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39378 ms - Host latency: 5.22296 ms (end to end 8.59287 ms, enqueue 0.840353 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39348 ms - Host latency: 5.22068 ms (end to end 8.59471 ms, enqueue 0.841458 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.3905 ms - Host latency: 5.2291 ms (end to end 8.58936 ms, enqueue 0.851166 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39153 ms - Host latency: 5.22668 ms (end to end 8.58445 ms, enqueue 0.846021 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39194 ms - Host latency: 5.23068 ms (end to end 8.58718 ms, enqueue 0.847473 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39655 ms - Host latency: 5.23341 ms (end to end 8.59748 ms, enqueue 0.851154 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.38938 ms - Host latency: 5.22511 ms (end to end 8.58676 ms, enqueue 0.847327 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.38999 ms - Host latency: 5.21852 ms (end to end 8.58385 ms, enqueue 0.839435 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.38671 ms - Host latency: 5.21207 ms (end to end 8.58193 ms, enqueue 0.836719 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39061 ms - Host latency: 5.21068 ms (end to end 8.58914 ms, enqueue 0.839465 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.38928 ms - Host latency: 5.20912 ms (end to end 8.59262 ms, enqueue 0.831183 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.3903 ms - Host latency: 5.21839 ms (end to end 8.58528 ms, enqueue 0.83764 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39163 ms - Host latency: 5.21846 ms (end to end 8.57381 ms, enqueue 0.841345 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39656 ms - Host latency: 5.21716 ms (end to end 8.57826 ms, enqueue 0.830664 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.42156 ms - Host latency: 5.24039 ms (end to end 8.62949 ms, enqueue 0.833093 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.43372 ms - Host latency: 5.25287 ms (end to end 8.69987 ms, enqueue 0.83324 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.43312 ms - Host latency: 5.25192 ms (end to end 8.6694 ms, enqueue 0.832214 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.41146 ms - Host latency: 5.23201 ms (end to end 8.60663 ms, enqueue 0.82572 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.41285 ms - Host latency: 5.23218 ms (end to end 8.61136 ms, enqueue 0.832764 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.40012 ms - Host latency: 5.2196 ms (end to end 8.58727 ms, enqueue 0.850647 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39543 ms - Host latency: 5.21531 ms (end to end 8.57894 ms, enqueue 0.827368 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.38906 ms - Host latency: 5.20818 ms (end to end 8.56248 ms, enqueue 0.828931 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.3907 ms - Host latency: 5.21016 ms (end to end 8.56539 ms, enqueue 0.827527 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39103 ms - Host latency: 5.21047 ms (end to end 8.56284 ms, enqueue 0.825366 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.38647 ms - Host latency: 5.20684 ms (end to end 8.55565 ms, enqueue 0.832361 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.38813 ms - Host latency: 5.20767 ms (end to end 8.5603 ms, enqueue 0.826392 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.38833 ms - Host latency: 5.20817 ms (end to end 8.56062 ms, enqueue 0.82644 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.38877 ms - Host latency: 5.2087 ms (end to end 8.55929 ms, enqueue 0.827551 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39008 ms - Host latency: 5.20935 ms (end to end 8.56344 ms, enqueue 0.827515 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.38998 ms - Host latency: 5.20896 ms (end to end 8.56556 ms, enqueue 0.82417 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39059 ms - Host latency: 5.21053 ms (end to end 8.56642 ms, enqueue 0.828955 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.38937 ms - Host latency: 5.20872 ms (end to end 8.5644 ms, enqueue 0.830273 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.38773 ms - Host latency: 5.20697 ms (end to end 8.56074 ms, enqueue 0.830835 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.38937 ms - Host latency: 5.21057 ms (end to end 8.56193 ms, enqueue 0.828564 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39233 ms - Host latency: 5.22742 ms (end to end 8.5647 ms, enqueue 0.84176 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.38998 ms - Host latency: 5.22927 ms (end to end 8.56314 ms, enqueue 0.839661 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.40747 ms - Host latency: 5.23805 ms (end to end 8.59196 ms, enqueue 0.836328 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.41411 ms - Host latency: 5.25282 ms (end to end 8.60865 ms, enqueue 0.842517 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.41072 ms - Host latency: 5.23074 ms (end to end 8.6022 ms, enqueue 0.827246 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.4137 ms - Host latency: 5.26472 ms (end to end 8.60649 ms, enqueue 0.850732 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.41343 ms - Host latency: 5.26509 ms (end to end 8.60791 ms, enqueue 0.848242 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.4114 ms - Host latency: 5.23052 ms (end to end 8.60869 ms, enqueue 0.827124 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.4125 ms - Host latency: 5.23301 ms (end to end 8.61277 ms, enqueue 0.828491 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.41221 ms - Host latency: 5.23335 ms (end to end 8.60691 ms, enqueue 0.830859 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.41121 ms - Host latency: 5.23301 ms (end to end 8.60623 ms, enqueue 0.830713 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.41077 ms - Host latency: 5.2311 ms (end to end 8.60837 ms, enqueue 0.831592 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.41296 ms - Host latency: 5.23372 ms (end to end 8.6106 ms, enqueue 0.826538 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.4106 ms - Host latency: 5.22937 ms (end to end 8.60547 ms, enqueue 0.830493 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.41052 ms - Host latency: 5.22947 ms (end to end 8.60701 ms, enqueue 0.828101 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.41296 ms - Host latency: 5.23491 ms (end to end 8.61189 ms, enqueue 0.825537 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39163 ms - Host latency: 5.21104 ms (end to end 8.57449 ms, enqueue 0.826563 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39526 ms - Host latency: 5.21514 ms (end to end 8.57009 ms, enqueue 0.826392 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39834 ms - Host latency: 5.21846 ms (end to end 8.58147 ms, enqueue 0.828711 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39312 ms - Host latency: 5.21267 ms (end to end 8.56982 ms, enqueue 0.826392 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.38918 ms - Host latency: 5.20937 ms (end to end 8.56243 ms, enqueue 0.825342 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.38728 ms - Host latency: 5.2075 ms (end to end 8.56199 ms, enqueue 0.826636 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39062 ms - Host latency: 5.21123 ms (end to end 8.56453 ms, enqueue 0.829004 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39016 ms - Host latency: 5.21223 ms (end to end 8.56665 ms, enqueue 0.829224 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.38777 ms - Host latency: 5.20757 ms (end to end 8.55845 ms, enqueue 0.82832 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.38635 ms - Host latency: 5.20569 ms (end to end 8.57888 ms, enqueue 0.812671 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.3877 ms - Host latency: 5.20898 ms (end to end 8.56001 ms, enqueue 0.82937 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.39055 ms - Host latency: 5.21072 ms (end to end 8.56577 ms, enqueue 0.829712 ms)\n",
      "[06/02/2022-23:00:22] [I] Average on 10 runs - GPU latency: 4.40457 ms - Host latency: 5.22507 ms (end to end 8.59529 ms, enqueue 0.829712 ms)\n",
      "[06/02/2022-23:00:22] [I] \n",
      "[06/02/2022-23:00:22] [I] === Performance summary ===\n",
      "[06/02/2022-23:00:22] [I] Throughput: 227.028 qps\n",
      "[06/02/2022-23:00:22] [I] Latency: min = 5.17273 ms, max = 5.31128 ms, mean = 5.22095 ms, median = 5.21375 ms, percentile(99%) = 5.2937 ms\n",
      "[06/02/2022-23:00:22] [I] End-to-End Host Latency: min = 5.40649 ms, max = 8.74231 ms, mean = 8.57593 ms, median = 8.58069 ms, percentile(99%) = 8.70093 ms\n",
      "[06/02/2022-23:00:22] [I] Enqueue Time: min = 0.761169 ms, max = 0.969727 ms, mean = 0.833431 ms, median = 0.8255 ms, percentile(99%) = 0.893341 ms\n",
      "[06/02/2022-23:00:22] [I] H2D Latency: min = 0.800476 ms, max = 0.880554 ms, mean = 0.815253 ms, median = 0.810059 ms, percentile(99%) = 0.875244 ms\n",
      "[06/02/2022-23:00:22] [I] GPU Compute Time: min = 4.35609 ms, max = 4.44519 ms, mean = 4.39647 ms, median = 4.39294 ms, percentile(99%) = 4.43591 ms\n",
      "[06/02/2022-23:00:22] [I] D2H Latency: min = 0.0078125 ms, max = 0.0112305 ms, mean = 0.00923114 ms, median = 0.00915527 ms, percentile(99%) = 0.0107422 ms\n",
      "[06/02/2022-23:00:22] [I] Total Host Walltime: 3.01284 s\n",
      "[06/02/2022-23:00:22] [I] Total GPU Compute Time: 3.00718 s\n",
      "[06/02/2022-23:00:22] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[06/02/2022-23:00:22] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8204] # trtexec --onnx=resnet50_onnx_model_tf.onnx --saveEngine=resnet_engine.trt --explicitBatch --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw --fp16\n"
     ]
    }
   ],
   "source": [
    "# May need to shut down all kernels and restart before this - otherwise you might get cuDNN initialization errors:\n",
    "if USE_FP16:\n",
    "    !trtexec --onnx=resnet50_onnx_model_tf.onnx --saveEngine=resnet_engine.trt  --explicitBatch --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw --fp16\n",
    "else:\n",
    "    !trtexec --onnx=resnet50_onnx_model_tf.onnx --saveEngine=resnet_engine.trt  --explicitBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/02/2022-23:02:45] [TRT] [W] TensorRT was linked against cuBLAS/cuBLASLt 11.6.5 but loaded cuBLAS/cuBLASLt 11.5.2\n",
      "[06/02/2022-23:02:46] [TRT] [W] TensorRT was linked against cuBLAS/cuBLASLt 11.6.5 but loaded cuBLAS/cuBLASLt 11.5.2\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "f = open(\"resnet_engine.trt\", \"rb\")\n",
    "runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING)) \n",
    "\n",
    "engine = runtime.deserialize_cuda_engine(f.read())\n",
    "context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.empty([BATCH_SIZE, 1000], dtype = target_dtype) # Need to set output dtype to FP16 to enable FP16\n",
    "\n",
    "# Allocate device memory\n",
    "d_input = cuda.mem_alloc(1 * input_batch.nbytes)\n",
    "d_output = cuda.mem_alloc(1 * output.nbytes)\n",
    "\n",
    "bindings = [int(d_input), int(d_output)]\n",
    "\n",
    "stream = cuda.Stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(batch): # result gets copied into output\n",
    "    # Transfer input data to device\n",
    "    cuda.memcpy_htod_async(d_input, batch, stream)\n",
    "    # Execute model\n",
    "    context.execute_async_v2(bindings, stream.handle, None)\n",
    "    # Transfer predictions back\n",
    "    cuda.memcpy_dtoh_async(output, d_output, stream)\n",
    "    # Syncronize threads\n",
    "    stream.synchronize()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up...\n",
      "Done warming up!\n"
     ]
    }
   ],
   "source": [
    "print(\"Warming up...\")\n",
    "\n",
    "trt_predictions = predict(input_batch).astype(np.float32)\n",
    "\n",
    "print(\"Done warming up!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class | Probability (out of 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(160, 0.30859375),\n",
       " (169, 0.24609375),\n",
       " (212, 0.19018555),\n",
       " (170, 0.078063965),\n",
       " (207, 0.03152466)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = (-trt_predictions[0]).argsort()[:5]\n",
    "print(\"Class | Probability (out of 1)\")\n",
    "list(zip(indices, trt_predictions[0][indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.86 ms ± 18.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "_ = predict(input_batch) # Check TRT performance"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f49cab0004cfed5e72baf7f02cb7b5e0925b1cb329d6f2cfb632e4d853e97ccb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf26_trt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
