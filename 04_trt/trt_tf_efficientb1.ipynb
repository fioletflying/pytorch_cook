{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"/data/study/git/pytorch_cook/04_trt/data/b1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 96, 62, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "# from matplotlib import pyplot as plt\n",
    "BATCH_SIZE = 1024\n",
    "url='06l2_test.png'\n",
    "img = resize(io.imread(url), (96, 62, 1))\n",
    "input_batch = 255*np.array(np.repeat(np.expand_dims(np.array(img, dtype=np.float32), axis=0), BATCH_SIZE, axis=0), dtype=np.float32)\n",
    "\n",
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 09:23:12.639252: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201\n",
      "2022-06-09 09:23:13.749048: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-09 09:23:15.111926: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class | Probability (out of 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(13, 0.98952967),\n",
       " (3, 0.008999778),\n",
       " (0, 0.00051976135),\n",
       " (17, 0.0002356667),\n",
       " (9, 0.00013105824)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(input_batch) # warm up\n",
    "indices = (-predictions[0]).argsort()[:5]\n",
    "print(\"Class | Probability (out of 1)\")\n",
    "list(zip(indices, predictions[0][indices]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 ms ± 6.02 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "result = model.predict_on_batch(input_batch) # Check default performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 09:24:00.557980: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/tf26_trt/lib/python3.9/site-packages/tensorflow/python/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/tf26_trt/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-06-09 09:24:26,730 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-06-09 09:24:27,150 - WARNING - Importing a function (__inference_block5b_se_reduce_layer_call_and_return_conditional_losses_31609) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:27,157 - WARNING - Importing a function (__inference_block6a_expand_activation_layer_call_and_return_conditional_losses_62030) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:27,187 - WARNING - Importing a function (__inference_block6b_activation_layer_call_and_return_conditional_losses_62675) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:27,248 - WARNING - Importing a function (__inference_block5c_expand_activation_layer_call_and_return_conditional_losses_31741) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:27,344 - WARNING - Importing a function (__inference_block2c_activation_layer_call_and_return_conditional_losses_55674) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:27,357 - WARNING - Importing a function (__inference_block6d_expand_activation_layer_call_and_return_conditional_losses_32772) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:27,367 - WARNING - Importing a function (__inference_block5b_activation_layer_call_and_return_conditional_losses_31574) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:27,436 - WARNING - Importing a function (__inference_block3c_activation_layer_call_and_return_conditional_losses_30349) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:27,441 - WARNING - Importing a function (__inference_block6d_se_reduce_layer_call_and_return_conditional_losses_32849) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:27,466 - WARNING - Importing a function (__inference_block6a_se_reduce_layer_call_and_return_conditional_losses_62213) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:27,527 - WARNING - Importing a function (__inference_block6d_activation_layer_call_and_return_conditional_losses_32814) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:27,572 - WARNING - Importing a function (__inference_block4d_activation_layer_call_and_return_conditional_losses_59451) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:27,756 - WARNING - Importing a function (__inference_block6c_activation_layer_call_and_return_conditional_losses_32605) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:27,856 - WARNING - Importing a function (__inference_block5a_expand_activation_layer_call_and_return_conditional_losses_59865) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:27,873 - WARNING - Importing a function (__inference_block6e_expand_activation_layer_call_and_return_conditional_losses_32981) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:27,878 - WARNING - Importing a function (__inference_block2c_activation_layer_call_and_return_conditional_losses_29736) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:27,917 - WARNING - Importing a function (__inference_block6c_se_reduce_layer_call_and_return_conditional_losses_32640) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:27,963 - WARNING - Importing a function (__inference_block4a_se_reduce_layer_call_and_return_conditional_losses_57883) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:27,986 - WARNING - Importing a function (__inference_block1a_se_reduce_layer_call_and_return_conditional_losses_53753) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:27,991 - WARNING - Importing a function (__inference_block5d_expand_activation_layer_call_and_return_conditional_losses_61477) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:28,088 - WARNING - Importing a function (__inference_block7b_se_reduce_layer_call_and_return_conditional_losses_33461) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:28,154 - WARNING - Importing a function (__inference_block5b_activation_layer_call_and_return_conditional_losses_60510) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:28,181 - WARNING - Importing a function (__inference_block6e_activation_layer_call_and_return_conditional_losses_64334) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:28,185 - WARNING - Importing a function (__inference_block3c_expand_activation_layer_call_and_return_conditional_losses_57147) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:28,195 - WARNING - Importing a function (__inference_block2b_se_reduce_layer_call_and_return_conditional_losses_55165) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:28,227 - WARNING - Importing a function (__inference_block3a_se_reduce_layer_call_and_return_conditional_losses_56271) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:28,258 - WARNING - Importing a function (__inference_block4d_expand_activation_layer_call_and_return_conditional_losses_31129) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:28,312 - WARNING - Importing a function (__inference_block6b_expand_activation_layer_call_and_return_conditional_losses_32354) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:28,331 - WARNING - Importing a function (__inference_block5d_activation_layer_call_and_return_conditional_losses_31992) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:28,337 - WARNING - Importing a function (__inference_block5b_se_reduce_layer_call_and_return_conditional_losses_60554) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:28,342 - WARNING - Importing a function (__inference_block2a_se_reduce_layer_call_and_return_conditional_losses_29368) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:28,690 - WARNING - Importing a function (__inference_block3b_expand_activation_layer_call_and_return_conditional_losses_56594) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:28,768 - WARNING - Importing a function (__inference_block4a_activation_layer_call_and_return_conditional_losses_57839) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:28,771 - WARNING - Importing a function (__inference_block4c_activation_layer_call_and_return_conditional_losses_30962) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:28,785 - WARNING - Importing a function (__inference_block2c_expand_activation_layer_call_and_return_conditional_losses_29694) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:28,809 - WARNING - Importing a function (__inference_block2a_expand_activation_layer_call_and_return_conditional_losses_54476) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:28,814 - WARNING - Importing a function (__inference_block7b_expand_activation_layer_call_and_return_conditional_losses_33384) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:28,860 - WARNING - Importing a function (__inference_block3c_se_reduce_layer_call_and_return_conditional_losses_57330) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:29,016 - WARNING - Importing a function (__inference_block1b_activation_layer_call_and_return_conditional_losses_29123) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:29,027 - WARNING - Importing a function (__inference_block6b_se_reduce_layer_call_and_return_conditional_losses_32431) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:29,126 - WARNING - Importing a function (__inference_stem_activation_layer_call_and_return_conditional_losses_28937) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:29,140 - WARNING - Importing a function (__inference_block5d_activation_layer_call_and_return_conditional_losses_61616) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:29,143 - WARNING - Importing a function (__inference_block4a_se_reduce_layer_call_and_return_conditional_losses_30594) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:29,162 - WARNING - Importing a function (__inference_block7b_activation_layer_call_and_return_conditional_losses_33426) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:29,182 - WARNING - Importing a function (__inference_block6a_se_reduce_layer_call_and_return_conditional_losses_32237) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:29,216 - WARNING - Importing a function (__inference_block7a_activation_layer_call_and_return_conditional_losses_64887) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:29,227 - WARNING - Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_64748) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:29,238 - WARNING - Importing a function (__inference_block6d_activation_layer_call_and_return_conditional_losses_63781) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:29,272 - WARNING - Importing a function (__inference_block3b_activation_layer_call_and_return_conditional_losses_56733) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:29,293 - WARNING - Importing a function (__inference_block6a_activation_layer_call_and_return_conditional_losses_32202) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:29,298 - WARNING - Importing a function (__inference_top_activation_layer_call_and_return_conditional_losses_33593) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:29,336 - WARNING - Importing a function (__inference_block7a_se_reduce_layer_call_and_return_conditional_losses_64931) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:29,341 - WARNING - Importing a function (__inference_block6e_se_reduce_layer_call_and_return_conditional_losses_64378) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:29,433 - WARNING - Importing a function (__inference_block6a_expand_activation_layer_call_and_return_conditional_losses_32159) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:29,479 - WARNING - Importing a function (__inference_model_layer_call_and_return_conditional_losses_50867) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:30,134 - WARNING - Importing a function (__inference_block3c_activation_layer_call_and_return_conditional_losses_57286) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:30,188 - WARNING - Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_33190) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:30,205 - WARNING - Importing a function (__inference_block6b_expand_activation_layer_call_and_return_conditional_losses_62536) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:30,267 - WARNING - Importing a function (__inference_block7b_se_reduce_layer_call_and_return_conditional_losses_65437) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:30,471 - WARNING - Importing a function (__inference_block3a_activation_layer_call_and_return_conditional_losses_56227) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:30,484 - WARNING - Importing a function (__inference_block3a_se_reduce_layer_call_and_return_conditional_losses_29981) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:30,557 - WARNING - Importing a function (__inference_model_layer_call_and_return_conditional_losses_53404) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:31,159 - WARNING - Importing a function (__inference_block3b_se_reduce_layer_call_and_return_conditional_losses_30175) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:31,187 - WARNING - Importing a function (__inference_block3a_activation_layer_call_and_return_conditional_losses_29946) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:31,193 - WARNING - Importing a function (__inference_block5a_expand_activation_layer_call_and_return_conditional_losses_31338) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:31,245 - WARNING - Importing a function (__inference_block5c_activation_layer_call_and_return_conditional_losses_61063) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:31,260 - WARNING - Importing a function (__inference_block5c_expand_activation_layer_call_and_return_conditional_losses_60924) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:31,275 - WARNING - Importing a function (__inference_block2b_expand_activation_layer_call_and_return_conditional_losses_29485) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:31,287 - WARNING - Importing a function (__inference_block4c_se_reduce_layer_call_and_return_conditional_losses_58942) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:31,301 - WARNING - Importing a function (__inference_block6e_expand_activation_layer_call_and_return_conditional_losses_64195) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:31,309 - WARNING - Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_30920) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:31,336 - WARNING - Importing a function (__inference_block5d_se_reduce_layer_call_and_return_conditional_losses_32027) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:31,365 - WARNING - Importing a function (__inference_block5c_se_reduce_layer_call_and_return_conditional_losses_31818) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:31,413 - WARNING - Importing a function (__inference_block6e_se_reduce_layer_call_and_return_conditional_losses_33058) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:31,491 - WARNING - Importing a function (__inference_block4c_se_reduce_layer_call_and_return_conditional_losses_30997) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:31,566 - WARNING - Importing a function (__inference_block7b_expand_activation_layer_call_and_return_conditional_losses_65254) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:31,574 - WARNING - Importing a function (__inference_block5d_se_reduce_layer_call_and_return_conditional_losses_61660) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:31,859 - WARNING - Importing a function (__inference_block3c_se_reduce_layer_call_and_return_conditional_losses_30384) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:31,964 - WARNING - Importing a function (__inference_block4b_expand_activation_layer_call_and_return_conditional_losses_58206) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,030 - WARNING - Importing a function (__inference_block4b_expand_activation_layer_call_and_return_conditional_losses_30711) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,035 - WARNING - Importing a function (__inference_block6d_se_reduce_layer_call_and_return_conditional_losses_63825) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,045 - WARNING - Importing a function (__inference_block7a_activation_layer_call_and_return_conditional_losses_33232) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,095 - WARNING - Importing a function (__inference_block3a_expand_activation_layer_call_and_return_conditional_losses_29903) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,127 - WARNING - Importing a function (__inference_block5c_se_reduce_layer_call_and_return_conditional_losses_61107) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,140 - WARNING - Importing a function (__inference_block6a_activation_layer_call_and_return_conditional_losses_62169) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,157 - WARNING - Importing a function (__inference_block6b_activation_layer_call_and_return_conditional_losses_32396) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,162 - WARNING - Importing a function (__inference_block4c_activation_layer_call_and_return_conditional_losses_58898) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,209 - WARNING - Importing a function (__inference_block1a_se_reduce_layer_call_and_return_conditional_losses_29014) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,276 - WARNING - Importing a function (__inference_block6b_se_reduce_layer_call_and_return_conditional_losses_62719) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,356 - WARNING - Importing a function (__inference_block4b_activation_layer_call_and_return_conditional_losses_58345) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,359 - WARNING - Importing a function (__inference_block4a_activation_layer_call_and_return_conditional_losses_30559) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,390 - WARNING - Importing a function (__inference_block1a_activation_layer_call_and_return_conditional_losses_53709) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,450 - WARNING - Importing a function (__inference_block4b_se_reduce_layer_call_and_return_conditional_losses_30788) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,509 - WARNING - Importing a function (__inference_block7a_se_reduce_layer_call_and_return_conditional_losses_33267) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,527 - WARNING - Importing a function (__inference_block1b_activation_layer_call_and_return_conditional_losses_54062) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,534 - WARNING - Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_54615) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,574 - WARNING - Importing a function (__inference_block7b_activation_layer_call_and_return_conditional_losses_65393) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,609 - WARNING - Importing a function (__inference_block6d_expand_activation_layer_call_and_return_conditional_losses_63642) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,642 - WARNING - Importing a function (__inference_block1b_se_reduce_layer_call_and_return_conditional_losses_54106) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,669 - WARNING - Importing a function (__inference_block3b_se_reduce_layer_call_and_return_conditional_losses_56777) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,714 - WARNING - Importing a function (__inference_block3a_expand_activation_layer_call_and_return_conditional_losses_56088) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,816 - WARNING - Importing a function (__inference_block2b_activation_layer_call_and_return_conditional_losses_29527) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:32,832 - WARNING - Importing a function (__inference_block4b_activation_layer_call_and_return_conditional_losses_30753) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,002 - WARNING - Importing a function (__inference_block5b_expand_activation_layer_call_and_return_conditional_losses_31532) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,007 - WARNING - Importing a function (__inference_block4d_se_reduce_layer_call_and_return_conditional_losses_59495) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,102 - WARNING - Importing a function (__inference_block5a_se_reduce_layer_call_and_return_conditional_losses_31415) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,177 - WARNING - Importing a function (__inference_block2b_se_reduce_layer_call_and_return_conditional_losses_29562) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,201 - WARNING - Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_29333) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,222 - WARNING - Importing a function (__inference_block6c_expand_activation_layer_call_and_return_conditional_losses_32563) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,233 - WARNING - Importing a function (__inference_block6c_expand_activation_layer_call_and_return_conditional_losses_63089) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,273 - WARNING - Importing a function (__inference_block5b_expand_activation_layer_call_and_return_conditional_losses_60371) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,333 - WARNING - Importing a function (__inference_block2c_expand_activation_layer_call_and_return_conditional_losses_55535) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,347 - WARNING - Importing a function (__inference_block2b_expand_activation_layer_call_and_return_conditional_losses_54982) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,422 - WARNING - Importing a function (__inference_block4d_expand_activation_layer_call_and_return_conditional_losses_59312) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,437 - WARNING - Importing a function (__inference_block4d_activation_layer_call_and_return_conditional_losses_31171) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,482 - WARNING - Importing a function (__inference_block2c_se_reduce_layer_call_and_return_conditional_losses_55718) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,504 - WARNING - Importing a function (__inference_block3b_expand_activation_layer_call_and_return_conditional_losses_30098) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,573 - WARNING - Importing a function (__inference_block2a_expand_activation_layer_call_and_return_conditional_losses_29290) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,637 - WARNING - Importing a function (__inference_block5a_activation_layer_call_and_return_conditional_losses_31380) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,698 - WARNING - Importing a function (__inference_top_activation_layer_call_and_return_conditional_losses_65807) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,725 - WARNING - Importing a function (__inference_block4b_se_reduce_layer_call_and_return_conditional_losses_58389) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,792 - WARNING - Importing a function (__inference_block2a_se_reduce_layer_call_and_return_conditional_losses_54659) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:33,870 - WARNING - Importing a function (__inference_block1a_activation_layer_call_and_return_conditional_losses_28979) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:34,064 - WARNING - Importing a function (__inference_block4a_expand_activation_layer_call_and_return_conditional_losses_57700) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:34,099 - WARNING - Importing a function (__inference_stem_activation_layer_call_and_return_conditional_losses_53570) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:34,118 - WARNING - Importing a function (__inference_block5d_expand_activation_layer_call_and_return_conditional_losses_31950) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:34,136 - WARNING - Importing a function (__inference_block4d_se_reduce_layer_call_and_return_conditional_losses_31206) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:34,151 - WARNING - Importing a function (__inference_block5a_activation_layer_call_and_return_conditional_losses_60004) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:34,172 - WARNING - Importing a function (__inference_block5c_activation_layer_call_and_return_conditional_losses_31783) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:34,249 - WARNING - Importing a function (__inference_block1b_se_reduce_layer_call_and_return_conditional_losses_29158) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:34,337 - WARNING - Importing a function (__inference_block2c_se_reduce_layer_call_and_return_conditional_losses_29771) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:34,501 - WARNING - Importing a function (__inference_block5a_se_reduce_layer_call_and_return_conditional_losses_60048) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:34,505 - WARNING - Importing a function (__inference_block2b_activation_layer_call_and_return_conditional_losses_55121) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:34,550 - WARNING - Importing a function (__inference__wrapped_model_19379) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:35,125 - WARNING - Importing a function (__inference_block4a_expand_activation_layer_call_and_return_conditional_losses_30516) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:35,204 - WARNING - Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_58759) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:35,207 - WARNING - Importing a function (__inference_block3b_activation_layer_call_and_return_conditional_losses_30140) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:35,223 - WARNING - Importing a function (__inference_block3c_expand_activation_layer_call_and_return_conditional_losses_30307) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:35,228 - WARNING - Importing a function (__inference_block6c_se_reduce_layer_call_and_return_conditional_losses_63272) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:35,270 - WARNING - Importing a function (__inference_block6e_activation_layer_call_and_return_conditional_losses_33023) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:35,283 - WARNING - Importing a function (__inference_block6c_activation_layer_call_and_return_conditional_losses_63228) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-06-09 09:24:38,968 - INFO - Signatures found in model: [serving_default].\n",
      "2022-06-09 09:24:38,969 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-06-09 09:24:38,971 - INFO - Output names: ['dense']\n",
      "WARNING:tensorflow:From /root/miniconda3/envs/tf26_trt/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-06-09 09:24:41,700 - WARNING - From /root/miniconda3/envs/tf26_trt/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-06-09 09:24:42,698 - INFO - Using tensorflow=2.6.0, onnx=1.10.1, tf2onnx=1.9.3/55c08b\n",
      "2022-06-09 09:24:42,698 - INFO - Using opset <onnx, 9>\n",
      "2022-06-09 09:24:45,765 - INFO - Computed 0 values for constant folding\n",
      "2022-06-09 09:24:48,190 - INFO - Optimizing ONNX model\n",
      "2022-06-09 09:24:55,089 - INFO - After optimization: BatchNormalization -62 (69->7), Cast -46 (46->0), Concat -23 (23->0), Const -365 (631->266), GlobalAveragePool +24 (0->24), Identity -21 (21->0), ReduceMean -24 (24->0), Reshape -22 (46->24), Shape -23 (23->0), Slice -23 (23->0), Squeeze +1 (23->24), Transpose -391 (391->0), Unsqueeze -92 (92->0)\n",
      "2022-06-09 09:24:55,169 - INFO - \n",
      "2022-06-09 09:24:55,169 - INFO - Successfully converted TensorFlow model my_model to ONNX\n",
      "2022-06-09 09:24:55,169 - INFO - Model inputs: ['input_1']\n",
      "2022-06-09 09:24:55,169 - INFO - Model outputs: ['dense']\n",
      "2022-06-09 09:24:55,170 - INFO - ONNX model is saved at temp.onnx\n"
     ]
    }
   ],
   "source": [
    "model.save('my_model')\n",
    "!python -m tf2onnx.convert --saved-model my_model --output temp.onnx\n",
    "onnx_model = onnx.load_model('temp.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = onnx_model.graph.input\n",
    "for input in inputs:\n",
    "    dim1 = input.type.tensor_type.shape.dim[0]\n",
    "    dim1.dim_value = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done saving!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"resnet50_onnx_model_tf.onnx\"\n",
    "onnx.save_model(onnx_model, model_name)\n",
    "print(\"Done saving!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restarting kernel  in three seconds...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "print(\"Restarting kernel  in three seconds...\")\n",
    "time.sleep(3)\n",
    "print(\"Restarting kernel now\")\n",
    "os._exit(0) # Shut down all kernels so TRT doesn't fight with Tensorflow fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "USE_FP16 = False\n",
    "\n",
    "target_dtype = np.float16 if USE_FP16 else np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 96, 62, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "# from matplotlib import pyplot as plt\n",
    "BATCH_SIZE = 1024\n",
    "url='06l2_test.png'\n",
    "img = resize(io.imread(url), (96, 62, 1))\n",
    "input_batch = 255*np.array(np.repeat(np.expand_dims(np.array(img, dtype=np.float32), axis=0), BATCH_SIZE, axis=0), dtype=np.float32)\n",
    "\n",
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = input_batch.astype(target_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8204] # trtexec --onnx=resnet50_onnx_model_tf.onnx --saveEngine=resnet_engine.trt --explicitBatch\n",
      "[06/09/2022-09:26:26] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[06/09/2022-09:26:26] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[06/09/2022-09:26:26] [I] === Model Options ===\n",
      "[06/09/2022-09:26:26] [I] Format: ONNX\n",
      "[06/09/2022-09:26:26] [I] Model: resnet50_onnx_model_tf.onnx\n",
      "[06/09/2022-09:26:26] [I] Output:\n",
      "[06/09/2022-09:26:26] [I] === Build Options ===\n",
      "[06/09/2022-09:26:26] [I] Max batch: explicit batch\n",
      "[06/09/2022-09:26:26] [I] Workspace: 16 MiB\n",
      "[06/09/2022-09:26:26] [I] minTiming: 1\n",
      "[06/09/2022-09:26:26] [I] avgTiming: 8\n",
      "[06/09/2022-09:26:26] [I] Precision: FP32\n",
      "[06/09/2022-09:26:26] [I] Calibration: \n",
      "[06/09/2022-09:26:26] [I] Refit: Disabled\n",
      "[06/09/2022-09:26:26] [I] Sparsity: Disabled\n",
      "[06/09/2022-09:26:26] [I] Safe mode: Disabled\n",
      "[06/09/2022-09:26:26] [I] DirectIO mode: Disabled\n",
      "[06/09/2022-09:26:26] [I] Restricted mode: Disabled\n",
      "[06/09/2022-09:26:26] [I] Save engine: resnet_engine.trt\n",
      "[06/09/2022-09:26:26] [I] Load engine: \n",
      "[06/09/2022-09:26:26] [I] Profiling verbosity: 0\n",
      "[06/09/2022-09:26:26] [I] Tactic sources: Using default tactic sources\n",
      "[06/09/2022-09:26:26] [I] timingCacheMode: local\n",
      "[06/09/2022-09:26:26] [I] timingCacheFile: \n",
      "[06/09/2022-09:26:26] [I] Input(s)s format: fp32:CHW\n",
      "[06/09/2022-09:26:26] [I] Output(s)s format: fp32:CHW\n",
      "[06/09/2022-09:26:26] [I] Input build shapes: model\n",
      "[06/09/2022-09:26:26] [I] Input calibration shapes: model\n",
      "[06/09/2022-09:26:26] [I] === System Options ===\n",
      "[06/09/2022-09:26:26] [I] Device: 0\n",
      "[06/09/2022-09:26:26] [I] DLACore: \n",
      "[06/09/2022-09:26:26] [I] Plugins:\n",
      "[06/09/2022-09:26:26] [I] === Inference Options ===\n",
      "[06/09/2022-09:26:26] [I] Batch: Explicit\n",
      "[06/09/2022-09:26:26] [I] Input inference shapes: model\n",
      "[06/09/2022-09:26:26] [I] Iterations: 10\n",
      "[06/09/2022-09:26:26] [I] Duration: 3s (+ 200ms warm up)\n",
      "[06/09/2022-09:26:26] [I] Sleep time: 0ms\n",
      "[06/09/2022-09:26:26] [I] Idle time: 0ms\n",
      "[06/09/2022-09:26:26] [I] Streams: 1\n",
      "[06/09/2022-09:26:26] [I] ExposeDMA: Disabled\n",
      "[06/09/2022-09:26:26] [I] Data transfers: Enabled\n",
      "[06/09/2022-09:26:26] [I] Spin-wait: Disabled\n",
      "[06/09/2022-09:26:26] [I] Multithreading: Disabled\n",
      "[06/09/2022-09:26:26] [I] CUDA Graph: Disabled\n",
      "[06/09/2022-09:26:26] [I] Separate profiling: Disabled\n",
      "[06/09/2022-09:26:26] [I] Time Deserialize: Disabled\n",
      "[06/09/2022-09:26:26] [I] Time Refit: Disabled\n",
      "[06/09/2022-09:26:26] [I] Skip inference: Disabled\n",
      "[06/09/2022-09:26:26] [I] Inputs:\n",
      "[06/09/2022-09:26:26] [I] === Reporting Options ===\n",
      "[06/09/2022-09:26:26] [I] Verbose: Disabled\n",
      "[06/09/2022-09:26:26] [I] Averages: 10 inferences\n",
      "[06/09/2022-09:26:26] [I] Percentile: 99\n",
      "[06/09/2022-09:26:26] [I] Dump refittable layers:Disabled\n",
      "[06/09/2022-09:26:26] [I] Dump output: Disabled\n",
      "[06/09/2022-09:26:26] [I] Profile: Disabled\n",
      "[06/09/2022-09:26:26] [I] Export timing to JSON file: \n",
      "[06/09/2022-09:26:26] [I] Export output to JSON file: \n",
      "[06/09/2022-09:26:26] [I] Export profile to JSON file: \n",
      "[06/09/2022-09:26:26] [I] \n",
      "[06/09/2022-09:26:28] [I] === Device Information ===\n",
      "[06/09/2022-09:26:28] [I] Selected Device: NVIDIA GeForce RTX 3090\n",
      "[06/09/2022-09:26:28] [I] Compute Capability: 8.6\n",
      "[06/09/2022-09:26:28] [I] SMs: 82\n",
      "[06/09/2022-09:26:28] [I] Compute Clock Rate: 1.695 GHz\n",
      "[06/09/2022-09:26:28] [I] Device Global Memory: 24268 MiB\n",
      "[06/09/2022-09:26:28] [I] Shared Memory per SM: 100 KiB\n",
      "[06/09/2022-09:26:28] [I] Memory Bus Width: 384 bits (ECC disabled)\n",
      "[06/09/2022-09:26:28] [I] Memory Clock Rate: 9.751 GHz\n",
      "[06/09/2022-09:26:28] [I] \n",
      "[06/09/2022-09:26:28] [I] TensorRT version: 8.2.4\n",
      "[06/09/2022-09:26:29] [I] [TRT] [MemUsageChange] Init CUDA: CPU +459, GPU +0, now: CPU 471, GPU 489 (MiB)\n",
      "[06/09/2022-09:26:30] [I] [TRT] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 471 MiB, GPU 489 MiB\n",
      "[06/09/2022-09:26:30] [I] [TRT] [MemUsageSnapshot] End constructing builder kernel library: CPU 625 MiB, GPU 533 MiB\n",
      "[06/09/2022-09:26:30] [I] Start parsing network model\n",
      "[06/09/2022-09:26:30] [I] [TRT] ----------------------------------------------------------------\n",
      "[06/09/2022-09:26:30] [I] [TRT] Input filename:   resnet50_onnx_model_tf.onnx\n",
      "[06/09/2022-09:26:30] [I] [TRT] ONNX IR version:  0.0.4\n",
      "[06/09/2022-09:26:30] [I] [TRT] Opset version:    9\n",
      "[06/09/2022-09:26:30] [I] [TRT] Producer name:    tf2onnx\n",
      "[06/09/2022-09:26:30] [I] [TRT] Producer version: 1.9.3\n",
      "[06/09/2022-09:26:30] [I] [TRT] Domain:           \n",
      "[06/09/2022-09:26:30] [I] [TRT] Model version:    0\n",
      "[06/09/2022-09:26:30] [I] [TRT] Doc string:       \n",
      "[06/09/2022-09:26:30] [I] [TRT] ----------------------------------------------------------------\n",
      "[06/09/2022-09:26:30] [W] [TRT] onnx2trt_utils.cpp:366: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[06/09/2022-09:26:30] [W] [TRT] ShapedWeights.cpp:173: Weights StatefulPartitionedCall/model/dense/MatMul/ReadVariableOp:0 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[06/09/2022-09:26:30] [I] Finish parsing network model\n",
      "[06/09/2022-09:26:34] [W] [TRT] TensorRT was linked against cuBLAS/cuBLASLt 11.6.5 but loaded cuBLAS/cuBLASLt 11.5.2\n",
      "[06/09/2022-09:26:34] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +750, GPU +318, now: CPU 1405, GPU 851 (MiB)\n",
      "[06/09/2022-09:26:37] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +669, GPU +344, now: CPU 2074, GPU 1195 (MiB)\n",
      "[06/09/2022-09:26:37] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[06/09/2022-09:26:53] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output.\n",
      "[06/09/2022-09:30:04] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[06/09/2022-09:30:04] [I] [TRT] Total Host Persistent Memory: 215680\n",
      "[06/09/2022-09:30:04] [I] [TRT] Total Device Persistent Memory: 22758400\n",
      "[06/09/2022-09:30:04] [I] [TRT] Total Scratch Memory: 1310720\n",
      "[06/09/2022-09:30:04] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 19 MiB, GPU 1139 MiB\n",
      "[06/09/2022-09:30:04] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 114.389ms to assign 5 blocks to 299 nodes requiring 1305804800 bytes.\n",
      "[06/09/2022-09:30:04] [I] [TRT] Total Activation Memory: 1305804800\n",
      "[06/09/2022-09:30:04] [W] [TRT] TensorRT was linked against cuBLAS/cuBLASLt 11.6.5 but loaded cuBLAS/cuBLASLt 11.5.2\n",
      "[06/09/2022-09:30:04] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 3081, GPU 1703 (MiB)\n",
      "[06/09/2022-09:30:04] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 3081, GPU 1713 (MiB)\n",
      "[06/09/2022-09:30:04] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +4, GPU +26, now: CPU 4, GPU 26 (MiB)\n",
      "[06/09/2022-09:30:04] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 3088, GPU 1651 (MiB)\n",
      "[06/09/2022-09:30:04] [I] [TRT] Loaded engine size: 28 MiB\n",
      "[06/09/2022-09:30:04] [W] [TRT] TensorRT was linked against cuBLAS/cuBLASLt 11.6.5 but loaded cuBLAS/cuBLASLt 11.5.2\n",
      "[06/09/2022-09:30:04] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 3105, GPU 1689 (MiB)\n",
      "[06/09/2022-09:30:04] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 3105, GPU 1697 (MiB)\n",
      "[06/09/2022-09:30:04] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +25, now: CPU 0, GPU 25 (MiB)\n",
      "[06/09/2022-09:30:04] [I] Engine built in 215.927 sec.\n",
      "[06/09/2022-09:30:04] [W] [TRT] TensorRT was linked against cuBLAS/cuBLASLt 11.6.5 but loaded cuBLAS/cuBLASLt 11.5.2\n",
      "[06/09/2022-09:30:04] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2897, GPU 1663 (MiB)\n",
      "[06/09/2022-09:30:04] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2897, GPU 1671 (MiB)\n",
      "[06/09/2022-09:30:04] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +1267, now: CPU 0, GPU 1292 (MiB)\n",
      "[06/09/2022-09:30:04] [I] Using random values for input input_1\n",
      "[06/09/2022-09:30:04] [I] Created input binding for input_1 with dimensions 1024x96x62x1\n",
      "[06/09/2022-09:30:04] [I] Using random values for output dense\n",
      "[06/09/2022-09:30:04] [I] Created output binding for dense with dimensions 1024x28\n",
      "[06/09/2022-09:30:04] [I] Starting inference\n",
      "[06/09/2022-09:30:08] [I] Warmup completed 5 queries over 200 ms\n",
      "[06/09/2022-09:30:08] [I] Timing trace has 69 queries over 3.14305 s\n",
      "[06/09/2022-09:30:08] [I] \n",
      "[06/09/2022-09:30:08] [I] === Trace details ===\n",
      "[06/09/2022-09:30:08] [I] Trace averages of 10 runs:\n",
      "[06/09/2022-09:30:08] [I] Average on 10 runs - GPU latency: 44.7982 ms - Host latency: 46.8395 ms (end to end 89.3663 ms, enqueue 4.80817 ms)\n",
      "[06/09/2022-09:30:08] [I] Average on 10 runs - GPU latency: 44.9284 ms - Host latency: 46.9764 ms (end to end 89.6315 ms, enqueue 4.8138 ms)\n",
      "[06/09/2022-09:30:08] [I] Average on 10 runs - GPU latency: 44.9072 ms - Host latency: 46.9492 ms (end to end 89.6028 ms, enqueue 4.80554 ms)\n",
      "[06/09/2022-09:30:08] [I] Average on 10 runs - GPU latency: 44.9093 ms - Host latency: 46.9538 ms (end to end 89.6161 ms, enqueue 4.80929 ms)\n",
      "[06/09/2022-09:30:08] [I] Average on 10 runs - GPU latency: 44.9197 ms - Host latency: 46.9682 ms (end to end 89.6078 ms, enqueue 4.79426 ms)\n",
      "[06/09/2022-09:30:08] [I] Average on 10 runs - GPU latency: 44.939 ms - Host latency: 46.9878 ms (end to end 89.6572 ms, enqueue 4.80425 ms)\n",
      "[06/09/2022-09:30:08] [I] \n",
      "[06/09/2022-09:30:08] [I] === Performance summary ===\n",
      "[06/09/2022-09:30:08] [I] Throughput: 21.9532 qps\n",
      "[06/09/2022-09:30:08] [I] Latency: min = 46.8058 ms, max = 47.0956 ms, mean = 46.9489 ms, median = 46.9745 ms, percentile(99%) = 47.0956 ms\n",
      "[06/09/2022-09:30:08] [I] End-to-End Host Latency: min = 89.3271 ms, max = 89.8083 ms, mean = 89.587 ms, median = 89.624 ms, percentile(99%) = 89.8083 ms\n",
      "[06/09/2022-09:30:08] [I] Enqueue Time: min = 4.71997 ms, max = 4.8728 ms, mean = 4.80499 ms, median = 4.80811 ms, percentile(99%) = 4.8728 ms\n",
      "[06/09/2022-09:30:08] [I] H2D Latency: min = 2.01782 ms, max = 2.04504 ms, mean = 2.0295 ms, median = 2.02808 ms, percentile(99%) = 2.04504 ms\n",
      "[06/09/2022-09:30:08] [I] GPU Compute Time: min = 44.7652 ms, max = 45.0508 ms, mean = 44.9034 ms, median = 44.9312 ms, percentile(99%) = 45.0508 ms\n",
      "[06/09/2022-09:30:08] [I] D2H Latency: min = 0.0117188 ms, max = 0.0180664 ms, mean = 0.0160098 ms, median = 0.0160522 ms, percentile(99%) = 0.0180664 ms\n",
      "[06/09/2022-09:30:08] [I] Total Host Walltime: 3.14305 s\n",
      "[06/09/2022-09:30:08] [I] Total GPU Compute Time: 3.09833 s\n",
      "[06/09/2022-09:30:08] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[06/09/2022-09:30:08] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8204] # trtexec --onnx=resnet50_onnx_model_tf.onnx --saveEngine=resnet_engine.trt --explicitBatch\n"
     ]
    }
   ],
   "source": [
    "# May need to shut down all kernels and restart before this - otherwise you might get cuDNN initialization errors:\n",
    "if USE_FP16:\n",
    "    !trtexec --onnx=resnet50_onnx_model_tf.onnx --saveEngine=resnet_engine.trt  --explicitBatch --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw --fp16\n",
    "else:\n",
    "    !trtexec --onnx=resnet50_onnx_model_tf.onnx --saveEngine=resnet_engine.trt  --explicitBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/09/2022-09:42:36] [TRT] [W] TensorRT was linked against cuBLAS/cuBLASLt 11.6.5 but loaded cuBLAS/cuBLASLt 11.5.2\n",
      "[06/09/2022-09:42:37] [TRT] [W] TensorRT was linked against cuBLAS/cuBLASLt 11.6.5 but loaded cuBLAS/cuBLASLt 11.5.2\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "f = open(\"resnet_engine.trt\", \"rb\")\n",
    "runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING)) \n",
    "\n",
    "engine = runtime.deserialize_cuda_engine(f.read())\n",
    "context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.empty([BATCH_SIZE, 22], dtype = target_dtype) # Need to set output dtype to FP16 to enable FP16\n",
    "\n",
    "# Allocate device memory\n",
    "d_input = cuda.mem_alloc(1 * input_batch.nbytes)\n",
    "d_output = cuda.mem_alloc(1 * output.nbytes)\n",
    "\n",
    "bindings = [int(d_input), int(d_output)]\n",
    "\n",
    "stream = cuda.Stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(batch): # result gets copied into output\n",
    "    # Transfer input data to device\n",
    "    cuda.memcpy_htod_async(d_input, batch, stream)\n",
    "    # Execute model\n",
    "    context.execute_async_v2(bindings, stream.handle, None)\n",
    "    # Transfer predictions back\n",
    "    cuda.memcpy_dtoh_async(output, d_output, stream)\n",
    "    # Syncronize threads\n",
    "    stream.synchronize()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up...\n",
      "Done warming up!\n"
     ]
    }
   ],
   "source": [
    "print(\"Warming up...\")\n",
    "\n",
    "trt_predictions = predict(input_batch).astype(np.float32)\n",
    "\n",
    "print(\"Done warming up!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class | Probability (out of 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(13, 0.9894561),\n",
       " (3, 0.009055261),\n",
       " (0, 0.0005255092),\n",
       " (17, 0.00023879894),\n",
       " (9, 0.00013224599)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = (-trt_predictions[0]).argsort()[:5]\n",
    "print(\"Class | Probability (out of 1)\")\n",
    "list(zip(indices, trt_predictions[0][indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.5 ms ± 31.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "_ = predict(input_batch) # Check TRT performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "result = model.predict_on_batch(input_batch) # Check default performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f49cab0004cfed5e72baf7f02cb7b5e0925b1cb329d6f2cfb632e4d853e97ccb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf26_trt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
