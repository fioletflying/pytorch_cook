{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n",
      "False\n",
      "0\n",
      "0.11.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "import torchvision\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy版\n",
    "同时一个三阶多项式来预测y=sin(x)的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 [6.22953421 6.14295877 6.05715161 ... 3.0037072  3.05216095 3.10110646]\n",
      "155 [3.94455685 3.89056732 3.83704235 ... 1.89982415 1.92964967 1.95976566]\n",
      "255 [2.51367811 2.48017972 2.44695612 ... 1.15768187 1.17538698 1.1932562 ]\n",
      "355 [1.58350267 1.56312736 1.54290837 ... 0.69114965 0.70133617 0.71161037]\n",
      "455 [0.98267178 0.97061106 0.95863415 ... 0.40136987 0.40697448 0.4126219 ]\n",
      "555 [0.59783226 0.59096298 0.58413441 ... 0.22429055 0.22717272 0.23007256]\n",
      "655 [0.35410177 0.350408   0.34673043 ... 0.11854267 0.11986778 0.12119755]\n",
      "755 [0.20210331 0.20029437 0.19848874 ... 0.05750884 0.05799725 0.05848461]\n",
      "855 [0.10935456 0.10861307 0.1078691  ... 0.02414477 0.02423331 0.02431938]\n",
      "955 [0.05455155 0.05436751 0.05417941 ... 0.00760254 0.00754893 0.00749376]\n",
      "1055 [0.02377578 0.02383754 0.02389603 ... 0.00103201 0.000982   0.00093254]\n",
      "1155 [0.00797915 0.00810202 0.00822364 ... 0.00015204 0.00018245 0.00021599]\n",
      "1255 [0.00131783 0.00139747 0.00147845 ... 0.00232399 0.00247002 0.00262187]\n",
      "1355 [4.96684077e-05 3.18524102e-05 1.80952824e-05 ... 5.95255603e-03\n",
      " 6.22507371e-03 6.50598219e-03]\n",
      "1455 [0.00180566 0.00166674 0.00153452 ... 0.01009968 0.01049596 0.01090295]\n",
      "1555 [0.00510943 0.00484387 0.00458742 ... 0.01423674 0.01474704 0.0152701 ]\n",
      "1655 [0.0090611  0.00867354 0.00829727 ... 0.01808641 0.01869794 0.01932398]\n",
      "1755 [0.0131299  0.01263027 0.01214381 ... 0.02152235 0.02222155 0.02293677]\n",
      "1855 [0.01701867 0.01641917 0.01583446 ... 0.02450625 0.02528007 0.02607117]\n",
      "1955 [0.02057579 0.01988911 0.01921861 ... 0.02704868 0.02788523 0.02874009]\n",
      "Result: y = 0.017172806463013057 + 0.8238429144470775 x + -0.002962594871602744 x^2 + -0.08865094517113273 x^3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# 需要预测的值以及输入的值\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# 随机初始会权重参数\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "# 学习率\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(2000):\n",
    "    # 前向传播：计算预测值\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # 损失函数：计算损失值并打印\n",
    "    loss = np.square(y_pred - y)\n",
    "    if t % 100 == 55:\n",
    "        print(t, loss)\n",
    "\n",
    "    # 反向传播计算参数的梯度值\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # 更新权重参数\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor 版本：\n",
    "numpy 数组和 PyTorch 张量之间的最大区别在于，PyTorch 张量可以在 CPU 或 GPU 上运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 160.5891571044922\n",
      "188 109.75704956054688\n",
      "288 75.97692108154297\n",
      "388 53.5190315246582\n",
      "488 38.58336639404297\n",
      "588 28.646530151367188\n",
      "688 22.03302001953125\n",
      "788 17.629478454589844\n",
      "888 14.696233749389648\n",
      "988 12.741451263427734\n",
      "1088 11.438112258911133\n",
      "1188 10.568672180175781\n",
      "1288 9.988371849060059\n",
      "1388 9.600838661193848\n",
      "1488 9.341889381408691\n",
      "1588 9.168758392333984\n",
      "1688 9.052924156188965\n",
      "1788 8.97536849975586\n",
      "1888 8.923412322998047\n",
      "1988 8.888579368591309\n",
      "Result: y = 0.004361842758953571 + 0.8497730493545532 x + -0.0007524921675212681 x^2 + -0.09233927726745605 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# 设置张量在CPU还是在GPU上运行\n",
    "dtype = torch.float\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# 需要预测的值以及输入的值\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 初始化权重参数\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "# 设置超参数\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 前向传播：计算预测值\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "    \n",
    "    # 损失函数计算loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 88:\n",
    "        print(t, loss)\n",
    "\n",
    "    # 反向传播：计算参数的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # 更新权重参数\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Autograd 来计算梯度值\n",
    "- 张量表示计算图的一个节点\n",
    "- x为张量，该张量自带一个requires_grad属性，当该属性设置为True的时候表示可以通过autograd的方式来计算梯度\n",
    "- x.grad来保存x张量下的梯度值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 3805.037841796875\n",
      "188 2520.091796875\n",
      "288 1670.126220703125\n",
      "388 1107.87353515625\n",
      "488 735.93310546875\n",
      "588 489.880859375\n",
      "688 327.1033020019531\n",
      "788 219.41323852539062\n",
      "888 148.16537475585938\n",
      "988 101.0260238647461\n",
      "1088 69.83599853515625\n",
      "1188 49.19840621948242\n",
      "1288 35.5422477722168\n",
      "1388 26.505373001098633\n",
      "1488 20.524967193603516\n",
      "1588 16.56706428527832\n",
      "1688 13.947528839111328\n",
      "1788 12.213678359985352\n",
      "1888 11.065972328186035\n",
      "1988 10.30620002746582\n",
      "Result: y = 0.005787726957350969 + 0.8204581141471863 x + -0.000998479314148426 x^2 + -0.08816948533058167 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# 设置张量在CPU还是在GPU上运行\n",
    "dtype = torch.float\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# 需要预测的值以及输入的值\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 设置反向传播需要计算对应参数对应的梯度值grad\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 前向传播：计算预测值\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # 损失函数计算loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 88:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 利用PyTorch de 自动梯度机制来进行反向传播计算\n",
    "    # 当调用backward后就会自动计算所有tensor中设置了requires_grad=true的张量的梯度\n",
    "    # 当调用a.grad, b.grad...就可以获得对应的张量的梯度的值，而且该值会自动保留\n",
    "    # 如果不清空，会累加上去\n",
    "    loss.backward()\n",
    "\n",
    "    # 需要使用torch.no_grad()\n",
    "    # 不希望在下一步的梯度计算中记录这些操作\n",
    "    with torch.no_grad():\n",
    "        # 调用 grad来获得对应的张量的梯度的值\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "         # 如果不清空，会累加上去   \n",
    "         # 将梯度设置为零，以便为下一个循环做好准备。 否则，我们的梯度会记录所有已发生操作的运行记录\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "        \n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义新的 Autograd 函数\n",
    "对于比较复杂的传播函数来说的，需要自己计算梯度公式，创建该对应的公式的自动反向传播的类，该类是继承自torch.autograd.Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 218.8590850830078\n",
      "188 150.64144897460938\n",
      "288 104.73460388183594\n",
      "388 73.75899505615234\n",
      "488 52.82105255126953\n",
      "588 38.650856018066406\n",
      "688 29.052364349365234\n",
      "788 22.546472549438477\n",
      "888 18.134544372558594\n",
      "988 15.141712188720703\n",
      "1088 13.11081314086914\n",
      "1188 11.73245620727539\n",
      "1288 10.796770095825195\n",
      "1388 10.1614990234375\n",
      "1488 9.730156898498535\n",
      "1588 9.437203407287598\n",
      "1688 9.238287925720215\n",
      "1788 9.103202819824219\n",
      "1888 9.011453628540039\n",
      "1988 8.949139595031738\n",
      "Result: y = -5.423830273798558e-09 + -2.208526849746704 * P3(1.3320399228078372e-09 + 0.2554861009120941 x)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# 自定义的autograd 函数\n",
    "# 这个是用来对于自己比较复杂的传播函数来说的\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    # 继承torch.autograd.Function\n",
    "    # 需要自己实现forward 和 backward\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx,input):\n",
    "        \"\"\"\n",
    "        输入：\n",
    "            input: 是一个输入的Tensor\n",
    "            ctx: 表示上下文的对象，用来存放反向传播计算的信息\n",
    "        输出：输出Tensor是前向传播的结果\n",
    "\n",
    "        ctx使用save_for_backward来保存反向传播的张量的结果\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        输入：\n",
    "            grad_output: 损失值的梯度对应输出\n",
    "            ctx: 表示上下文的对象，用来存放反向传播计算的信息\n",
    "        输出：将损失值的的梯度乘以input的梯度值\n",
    "\n",
    "        ctx使用save_for_backward来保存反向传播的张量的结果\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
    "\n",
    "# 设置张量在CPU还是在GPU上运行\n",
    "dtype = torch.float\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# 需要预测的值以及输入的值\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 设置反向传播需要计算对应参数对应的梯度值的属性requires_grad\n",
    "# 这里使用了full来初始化对应的参数，需要注意的是这个值不能偏离太远了\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    # 使用Function中apply来实现自己定义的方法\n",
    "    # 将这个方法设置为P3\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "\n",
    "    # 前向传播来计算预测值y_pred\n",
    "    # P3将会使用自己定义的autograd 操作来实现\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "\n",
    "        # 损失函数计算loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 88:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 利用PyTorch de 自动梯度机制来进行反向传播计算\n",
    "    # 当调用backward后就会自动计算所有tensor中设置了requires_grad=true的张量的梯度\n",
    "    # 当调用a.grad, b.grad...就可以获得对应的张量的梯度的值，而且该值会自动保留\n",
    "    # 如果不清空，会累加上去\n",
    "    loss.backward()\n",
    "\n",
    "    # 这里手动更新weights, 所以需要使用torch.no_grad()\n",
    "    # 如果是自动更新就不需要torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        # 调用 grad来获得对应的张量的梯度的值\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "         # 如果不清空，会累加上去\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "        \n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn构建模型的版本\n",
    "- 建立模型： 通过nn建立一个线性传播的网络\n",
    "- 构建损失函数： nn.MSELoss\n",
    "- 自动反向传播：backward\n",
    "- 更新模型参数：model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 160.43678283691406\n",
      "188 112.2736587524414\n",
      "288 79.49169921875\n",
      "388 57.152862548828125\n",
      "488 41.912803649902344\n",
      "588 31.503929138183594\n",
      "688 24.38653564453125\n",
      "788 19.51413917541504\n",
      "888 16.1748104095459\n",
      "988 13.883589744567871\n",
      "1088 12.309727668762207\n",
      "1188 11.227375030517578\n",
      "1288 10.482193946838379\n",
      "1388 9.968621253967285\n",
      "1488 9.614269256591797\n",
      "1588 9.369522094726562\n",
      "1688 9.200289726257324\n",
      "1788 9.083160400390625\n",
      "1888 9.002005577087402\n",
      "1988 8.945728302001953\n",
      "Result: y = 0.010208604857325554 + 0.8513708114624023 x + -0.0017611539224162698 x^2 + -0.09256654232740402 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# 需要预测的值以及输入的值\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 这里开始构建模型\n",
    "# 该模型是一个线性的神经网络模型\n",
    "# x.unsqueeze(-1) 将会组成一个张量维度为 (2000, 1)\n",
    "# p 张量的维度为（3,）但是当与维度为 (2000, 1)进行操作的时候\n",
    "# 通过广播机制扩展到(2000,3)\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# 定义模型为Sequential，表示按顺序进行的\n",
    "# 使用Linear表现使用线性函数，将会自带有权重参数和偏置值\n",
    "# 这里输入的是3表示有3神经元，输出是1\n",
    "#Flatten这里是展平到1D tensor,为了与y的shape进行匹配\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3,1),\n",
    "    torch.nn.Flatten(0,1)\n",
    ")\n",
    "\n",
    "# nn 自带的常用的损失函数\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "\n",
    "    # Module对象重载了__call__操作，这里可以像函数那样使用\n",
    "    # 当传入一个tensor的时候，这里也会自动返回一个tensor\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # 损失函数计算loss\n",
    "    loss = loss_fn(y_pred,y)\n",
    "    if t % 100 == 88:\n",
    "        print(t, loss.item())\n",
    "\n",
    "\n",
    "    # 将模型中反向传播计算的梯度值设置为零\n",
    "    # 这里是避免上一次的结果累加\n",
    "    model.zero_grad()\n",
    "\n",
    "    # 反向传播：将会计算模型中的参数对应的梯度\n",
    "    loss.backward()\n",
    "\n",
    "    # 这里手动更新weights, 所以需要使用torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "        \n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用optim来进行相关进行更新权重，Pytorch 优化器中已经集成很多较为常用的优化算法：SGD,RMSProp,Adam等\n",
    "- 梯度清零：optimizer.zero_grad() \n",
    "- 自动反向传播：loss.backward()\n",
    "- 更新权重：optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 95971.625\n",
      "188 95925.65625\n",
      "288 95884.1484375\n",
      "388 95844.2109375\n",
      "488 95804.296875\n",
      "588 95764.3828125\n",
      "688 95724.4921875\n",
      "788 95684.5859375\n",
      "888 95644.703125\n",
      "988 95604.8359375\n",
      "1088 95564.96875\n",
      "1188 95525.109375\n",
      "1288 95485.265625\n",
      "1388 95445.421875\n",
      "1488 95405.578125\n",
      "1588 95365.765625\n",
      "1688 95325.9453125\n",
      "1788 95286.125\n",
      "1888 95246.328125\n",
      "1988 95206.546875\n",
      "Result: y = -0.5215063095092773 + -0.35876762866973877 x + 0.5146413445472717 x^2 + -0.48071953654289246 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# 需要预测的值以及输入的值\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 这里开始构建模型\n",
    "# 该模型是一个线性的神经网络模型\n",
    "# x.unsqueeze(-1) 将会组成一个张量维度为 (2000, 1)\n",
    "# p 张量的维度为（3,）但是当与维度为 (2000, 1)进行操作的时候\n",
    "# 通过广播机制扩展到(2000,3)\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# 定义模型为Sequential，表示按顺序进行的\n",
    "# 使用Linear表现使用线性函数，将会自带有权重参数和偏置值\n",
    "# 这里输入的是3表示有3神经元，输出是1\n",
    "#Flatten这里是展平到1D tensor,为了与y的shape进行匹配\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3,1),\n",
    "    torch.nn.Flatten(0,1)\n",
    ")\n",
    "\n",
    "# nn 自带的常用的损失函数\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "#利用优化器来自动更新参数\n",
    "# 第一个参数：需要自动更新的参数张量\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "learning_rate = 1e-3\n",
    "\n",
    "for t in range(2000):\n",
    "\n",
    "    # Module对象重载了__call__操作，这里可以像函数那样使用\n",
    "    # 当传入一个tensor的时候，这里也会自动返回一个tensor\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # 损失函数计算loss\n",
    "    loss = loss_fn(y_pred,y)\n",
    "    if t % 100 == 88:\n",
    "        print(t, loss.item())\n",
    "\n",
    "\n",
    "    # 优化器将需要反向传播中张量的梯度值清理\n",
    "    # 因为梯度值会累积到缓存中\n",
    "    optimizer.zero_grad()\n",
    "    # model.zero_grad()\n",
    "\n",
    "    # 反向传播：将会计算模型中的参数对应的梯度\n",
    "    loss.backward()\n",
    "\n",
    "    # 优化器调用step()将会自动更新参数\n",
    "    optimizer.step()\n",
    "    # 这里手动更新weights, 所以需要使用torch.no_grad()\n",
    "    # with torch.no_grad():\n",
    "    #     for param in model.parameters():\n",
    "    #         param -= learning_rate * param.grad\n",
    "        \n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch：自定义nn模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 76240.4921875\n",
      "188 76201.359375\n",
      "288 76165.9609375\n",
      "388 76131.859375\n",
      "488 76098.1484375\n",
      "588 76064.4375\n",
      "688 76030.7421875\n",
      "788 75997.0546875\n",
      "888 75963.3671875\n",
      "988 75929.6953125\n",
      "1088 75896.0234375\n",
      "1188 75862.375\n",
      "1288 75828.7265625\n",
      "1388 75795.078125\n",
      "1488 75761.4453125\n",
      "1588 75727.8125\n",
      "1688 75694.203125\n",
      "1788 75660.5859375\n",
      "1888 75626.984375\n",
      "1988 75593.390625\n",
      "Result: y = y = 0.6693885326385498 + 1.1657319068908691 x + -0.17604434490203857 x^2 + 0.3843775689601898 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate four parameters and assign them as\n",
    "        member parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
    "# 需要预测的值以及输入的值\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 这里开始构建模型\n",
    "model = Polynomial3()\n",
    "\n",
    "# nn 自带的常用的损失函数\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "#利用优化器来自动更新参数\n",
    "# 第一个参数：需要自动更新的参数张量\n",
    "learning_rate = 1e-6\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for t in range(2000):\n",
    "\n",
    "    # Module对象重载了__call__操作，这里可以像函数那样使用\n",
    "    # 当传入一个tensor的时候，这里也会自动返回一个tensor\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 损失函数计算loss\n",
    "    loss = criterion(y_pred,y)\n",
    "    if t % 100 == 88:\n",
    "        print(t, loss.item())\n",
    "\n",
    "\n",
    "    # 优化器将需要反向传播中张量的梯度值清理\n",
    "    # 因为梯度值会累积到缓存中\n",
    "    optimizer.zero_grad()\n",
    "    # model.zero_grad()\n",
    "\n",
    "    # 反向传播：将会计算模型中的参数对应的梯度\n",
    "    loss.backward()\n",
    "\n",
    "    # 优化器调用step()将会自动更新参数\n",
    "    optimizer.step()\n",
    "        \n",
    "\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {model.string()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch：控制流 + 权重共享"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 1466.094970703125\n",
      "3999 746.9537353515625\n",
      "5999 346.7632751464844\n",
      "7999 172.6957550048828\n",
      "9999 87.82992553710938\n",
      "11999 44.260196685791016\n",
      "13999 26.115705490112305\n",
      "15999 16.93872833251953\n",
      "17999 12.612433433532715\n",
      "19999 10.663186073303223\n",
      "21999 9.607438087463379\n",
      "23999 9.23609733581543\n",
      "25999 8.787160873413086\n",
      "27999 8.930931091308594\n",
      "29999 8.887451171875\n",
      "Result: y = -0.006020872853696346 + 0.8541386723518372 x + 0.0005413547623902559 x^2 + -0.09321240335702896 x^3 + 1.148824958363548e-05 x^4 ? + 1.148824958363548e-05 x^5 ?\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate five parameters and assign them as members.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        self.e = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 4, 5\n",
    "        and reuse the e parameter to compute the contribution of these orders.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same parameter many\n",
    "        times when defining a computational graph.\n",
    "        \"\"\"\n",
    "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "        for exp in range(4, random.randint(4, 6)):\n",
    "            y = y + self.e * x ** exp\n",
    "        return y\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet()\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
    "for t in range(30000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 2000 == 1999:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2f00676180bb85528a787e87e45ca464fbafa4cd649a5b52b9e7fe827f38559"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('pt19': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
