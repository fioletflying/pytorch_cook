{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "GL_CLASSES = ['person', 'bird', 'cat', 'cow', 'dog', 'horse', 'sheep',\n",
    "           'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train',\n",
    "           'bottle', 'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor']\n",
    "GL_NUMBBOX = 2\n",
    "GL_NUMGRID = 7     \n",
    "STATIC_DEBUG = False  # 调试用  \n",
    "print(len(GL_CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(size, box):\n",
    "    \"\"\"\n",
    "    将标注数据中的左上角，右下角的坐标点的格式转换成 \n",
    "    中心点 + w,h，并进行归一化\n",
    "    \"\"\"\n",
    "    dw = 1. / size[0]\n",
    "    dh = 1. / size[1]\n",
    "    x = (box[0] + box[1]) / 2.0\n",
    "    y = (box[2] + box[3]) / 2.0\n",
    "    w = box[1] - box[0]\n",
    "    h = box[3] - box[2]\n",
    "    x = x * dw\n",
    "    y = y * dh\n",
    "    w = w * dw\n",
    "    h = h * dh\n",
    "    return (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 333\n",
      "1 2 3 4\n"
     ]
    }
   ],
   "source": [
    "in_file = \"/data/data/voc/VOCdevkit/VOC2007/Annotations/000007.xml\"\n",
    "tree = ET.parse(in_file)\n",
    "root = tree.getroot()\n",
    "size = root.find('size')\n",
    "w = int(size.find('width').text)\n",
    "h = int(size.find('height').text)\n",
    "\n",
    "print(w,h)\n",
    "bb = [1, 2, 3, 4]\n",
    "# 这里表示一个字符串的连接方式\n",
    "abc = \" \".join([str(a) for a in bb])\n",
    "print(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_annotation(anno_dir, image_id, labels_dir):\n",
    "    in_file = open(os.path.join(anno_dir,  'Annotations/%s' % (image_id)))\n",
    "    image_id = image_id.split('.')[0]\n",
    "    out_file = open(os.path.join(labels_dir, \"%s.txt\" %(image_id)), 'w')\n",
    "    tree = ET.parse(in_file)\n",
    "    root = tree.getroot()\n",
    "    size = root.find('size')\n",
    "    w = int(size.find('width').text)\n",
    "    h = int(size.find('height').text)\n",
    "\n",
    "    for obj in root.iter('object'):\n",
    "        difficult = obj.find('difficult').text\n",
    "        cls = obj.find('name').text\n",
    "        # 这里如何是比较难的物体就不执行检测\n",
    "        if cls not in GL_CLASSES or int(difficult) == 1:\n",
    "            continue\n",
    "        \n",
    "        cls_id = GL_CLASSES.index(cls)\n",
    "        xmlbox = obj.find('bndbox')\n",
    "        points = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), \n",
    "                    float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text))\n",
    "        bb = convert((w, h), points)\n",
    "        # 将序列中的元素以指定的字符连接生成一个新的字符串: 这里用空格来连接\n",
    "        # 后面接的是一个字符串的迭代器\n",
    "        out_file.write(str(cls_id) + \" \" + \" \".join([str(a) for a in bb]) + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label_txt(anno_dir, labels_dir):\n",
    "    file_names = os.listdir(os.path.join(anno_dir, 'Annotations'))\n",
    "    for file in file_names:\n",
    "        # print(file)\n",
    "        convert_annotation(anno_dir, file, labels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成对应的图片txt文档\n",
    "anno_dir = \"/data/data/voc/VOCdevkit/VOC2007\"\n",
    "labels_dir = \"/data/data/voc/VOCdevkit/VOC2007/labels\"\n",
    "# make_label_txt(anno_dir, labels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_labels_img(img_dir, label_dir, imgname):\n",
    "    img = cv2.imread(os.path.join(img_dir,imgname + \".jpg\"))\n",
    "    h, w = img.shape[:2]\n",
    "    print(\"width:%s, hegiht: %s\" %(w, h))\n",
    "    with open(os.path.join(label_dir, imgname + \".txt\"), 'r') as flabel:\n",
    "        for label in flabel:\n",
    "            label_info = label.split(' ')\n",
    "            label, x, y, box_w, box_h = [float(x.strip()) for x in label_info]\n",
    "            print(GL_CLASSES[int(label)])\n",
    "            pt1 = (int(x * w - box_w * w / 2), int(y * h - box_h * h / 2))\n",
    "            pt2 = (int(x * w + box_w * w / 2), int(y * h + box_h * h / 2))\n",
    "            cv2.putText(img, GL_CLASSES[int(label)], pt1, cv2.FONT_HERSHEY_SIMPLEX,0.5, (0,0,255))\n",
    "            cv2.rectangle(img, pt1, pt2, (0,0,255), 2)\n",
    "    img = img[...,::-1]\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试相关的数据\n",
    "img_dir = \"/data/data/voc/VOCdevkit/VOC2007/JPEGImages\"\n",
    "labels_dir = \"/data/data/voc/VOCdevkit/VOC2007/labels\"\n",
    "\n",
    "show_labels_img(img_dir, labels_dir, \"009950\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def img_augument(img_dir, save_img_dir, labels_dir):\n",
    "    imgs_list = [x.split('.')[0]+\".jpg\" for x in os.listdir(labels_dir)]\n",
    "    for img_name in imgs_list:\n",
    "        print(\"process %s\"%os.path.join(img_dir, img_name))\n",
    "        img = cv2.imread(os.path.join(img_dir, img_name))\n",
    "        h, w = img.shape[0:2]\n",
    "        input_size = 448  # 输入YOLOv1网络的图像尺寸为448x448\n",
    "        # 因为数据集内原始图像的尺寸是不定的，所以需要进行适当的padding，将原始图像padding成宽高一致的正方形\n",
    "        # 然后再将Padding后的正方形图像缩放成448x448\n",
    "        padw, padh = 0, 0  # 要记录宽高方向的padding具体数值，因为padding之后需要调整bbox的位置信息\n",
    "        if h > w:\n",
    "            padw = (h - w) // 2\n",
    "            img = np.pad(img, ((0, 0), (padw, padw), (0, 0)), 'constant', constant_values=0)\n",
    "        elif w > h:\n",
    "            padh = (w - h) // 2\n",
    "            img = np.pad(img, ((padh, padh), (0, 0), (0, 0)), 'constant', constant_values=0)\n",
    "        img = cv2.resize(img, (input_size, input_size))\n",
    "        cv2.imwrite(os.path.join(save_img_dir, img_name), img)\n",
    "        # 读取图像对应的bbox信息，按1维的方式储存，每5个元素表示一个bbox的(cls,xc,yc,w,h)\n",
    "        with open(os.path.join(labels_dir,img_name.split('.')[0] + \".txt\"), 'r') as f:\n",
    "            bbox = f.read().split('\\n')\n",
    "        bbox = [x.split() for x in bbox]\n",
    "        bbox = [float(x) for y in bbox for x in y]\n",
    "        if len(bbox) % 5 != 0:\n",
    "            raise ValueError(\"File:\"\n",
    "                             + os.path.join(labels_dir,img_name.split('.')[0] + \".txt\") + \"——bbox Extraction Error!\")\n",
    "\n",
    "        # 根据padding、图像增广等操作，将原始的bbox数据转换为修改后图像的bbox数据\n",
    "        if padw != 0:\n",
    "            for i in range(len(bbox) // 5):\n",
    "                bbox[i * 5 + 1] = (bbox[i * 5 + 1] * w + padw) / h\n",
    "                bbox[i * 5 + 3] = (bbox[i * 5 + 3] * w) / h\n",
    "                if STATIC_DEBUG:\n",
    "                    cv2.rectangle(img, (int(bbox[1] * input_size - bbox[3] * input_size / 2),\n",
    "                                        int(bbox[2] * input_size - bbox[4] * input_size / 2)),\n",
    "                                  (int(bbox[1] * input_size + bbox[3] * input_size / 2),\n",
    "                                   int(bbox[2] * input_size + bbox[4] * input_size / 2)), (0, 0, 255))\n",
    "        elif padh != 0:\n",
    "            for i in range(len(bbox) // 5):\n",
    "                bbox[i * 5 + 2] = (bbox[i * 5 + 2] * h + padh) / w\n",
    "                bbox[i * 5 + 4] = (bbox[i * 5 + 4] * h) / w\n",
    "                if STATIC_DEBUG:\n",
    "                    cv2.rectangle(img, (int(bbox[1] * input_size - bbox[3] * input_size / 2),\n",
    "                                        int(bbox[2] * input_size - bbox[4] * input_size / 2)),\n",
    "                                  (int(bbox[1] * input_size + bbox[3] * input_size / 2),\n",
    "                                   int(bbox[2] * input_size + bbox[4] * input_size / 2)), (0, 0, 255))\n",
    "        # 此处可以写代码验证一下，查看padding后修改的bbox数值是否正确，在原图中画出bbox检验\n",
    "        if STATIC_DEBUG:\n",
    "            cv2.imshow(\"bbox-%d\"%int(bbox[0]), img)\n",
    "            cv2.waitKey(0)\n",
    "        with open(os.path.join(labels_dir, img_name.split('.')[0] + \".txt\"), 'w') as f:\n",
    "            for i in range(len(bbox) // 5):\n",
    "                bbox = [str(x) for x in bbox[i*5:(i*5+5)]]\n",
    "                str_context = \" \".join(bbox)+'\\n'\n",
    "                f.write(str_context)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 3, 2)\n",
      "(8, 7, 2)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones((8,3,2))\n",
    "print(a.shape)\n",
    "h,w = a.shape[0:2]\n",
    "padw, padh = 0, 0\n",
    "if h > w:\n",
    "    padw = (h - w) // 2\n",
    "    a = np.pad(a, ((0,0), (padw, padw), (0, 0)), 'constant', constant_values=0)\n",
    "elif w > h:\n",
    "    padh = (w - h) // 2\n",
    "    a = np.pad(a, ((padh, padh), (0,0), (0,0)), 'constant', constant_values=0)\n",
    "print(a.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bbox2labels(bbox):\n",
    "    \"\"\"\n",
    "    将bbox的(cls,x,y,w,h)数据转换程训练时方便计算loss的数据形式(7,7,5*B+cls_num)\n",
    "    \"\"\"\n",
    "    # 这里默认是分成7个格子\n",
    "    gridesize = 1.0/7\n",
    "    # 初始化成(7,7,5*B+cls_num):个数不一样 和 类别不一样\n",
    "    labels = np.zeros((7,7,5*GL_NUMBBOX+len(GL_CLASSES)))\n",
    "    for i in range(len(bbox)//5):\n",
    "        # 计算在那个一格子里面\n",
    "        gridx = int(bbox[i * 5 + 1] // gridesize)\n",
    "        gridy = int(bbox[i * 5 + 2] // gridesize)\n",
    "        # 每一个目标的相对位置: (bbox中心坐标 - 网格左上角点的坐标)/网格大小\n",
    "        gridpx = bbox[i * 5 + 1] / gridesize - gridx\n",
    "        gridpy = bbox[i * 5 + 2] / gridesize - gridy\n",
    "        # 将第gridy行，gridx列的网格设置为负责当前ground truth的预测，置信度和对应类别概率均置为1\n",
    "        labels[gridy, gridx, 0:5]  = np.array([gridpx, gridpy, bbox[i * 5 + 3], bbox[i * 5 + 4], 1])\n",
    "        labels[gridy, gridx, 5:10] = np.array([gridpx, gridpy, bbox[i * 5 + 3], bbox[i * 5 + 4], 1])\n",
    "        labels[gridy, gridx, 10+int(bbox[i * 5])] = 1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_txt(img_dir, anno_dir, save_root_dir, train_val_ratio=0.9, padding=10, debug=False):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    将img_dir文件夹内的图片按实际需要处理后，存入save_dir\n",
    "    最终得到图片文件夹及所有图片对应的标注(train.csv/test.csv)和图片列表文件(train.txt, test.txt)\n",
    "    \"\"\"\n",
    "    labels_dir = os.path.join(anno_dir, \"labels\")\n",
    "    if not os.path.exists(labels_dir):\n",
    "        os.mkdir(labels_dir)\n",
    "        make_label_txt(anno_dir, labels_dir)\n",
    "        print(\"labels done.\")\n",
    "    save_img_dir = os.path.join(os.path.join(anno_dir, \"voc2012_forYolov1\"), \"img\")\n",
    "    if not os.path.exists(save_img_dir):\n",
    "        os.mkdir(save_img_dir)\n",
    "        # img_augument(img_dir, save_img_dir, labels_dir)\n",
    "    imgs_list = os.listdir(save_img_dir)\n",
    "    n_trainval = len(imgs_list)\n",
    "    shuffle_id = list(range(n_trainval))\n",
    "    random.shuffle(shuffle_id)\n",
    "    n_train = int(n_trainval*train_val_ratio)\n",
    "    train_id = shuffle_id[:n_train]\n",
    "    test_id = shuffle_id[n_train:]\n",
    "    traintxt = open(os.path.join(save_root_dir, \"train.txt\"), 'w')\n",
    "    traincsv = np.zeros((n_train, GL_NUMGRID*GL_NUMGRID*(5*GL_NUMBBOX+len(GL_CLASSES))),dtype=np.float32)\n",
    "    for i,id in enumerate(train_id):\n",
    "        img_name = imgs_list[id]\n",
    "        img_path = os.path.join(save_img_dir, img_name)+'\\n'\n",
    "        traintxt.write(img_path)\n",
    "        with open(os.path.join(labels_dir,\"%s.txt\"%img_name.split('.')[0]), 'r') as f:\n",
    "            bbox = [float(x) for x in f.read().split()]\n",
    "            traincsv[i,:] = convert_bbox2labels(bbox)\n",
    "    np.savetxt(os.path.join(save_root_dir, \"train.csv\"), traincsv)\n",
    "    print(\"Create %d train data.\" % (n_train))\n",
    "\n",
    "    testtxt = open(os.path.join(save_root_dir, \"test.txt\"), 'w')\n",
    "    testcsv = np.zeros((n_trainval - n_train, GL_NUMGRID*GL_NUMGRID*(5*GL_NUMBBOX+len(GL_CLASSES))),dtype=np.float32)\n",
    "    for i,id in enumerate(test_id):\n",
    "        img_name = imgs_list[id]\n",
    "        img_path = os.path.join(save_img_dir, img_name)+'\\n'\n",
    "        testtxt.write(img_path)\n",
    "        with open(os.path.join(labels_dir,\"%s.txt\"%img_name.split('.')[0]), 'r') as f:\n",
    "            bbox = [float(x) for x in f.read().split()]\n",
    "            testcsv[i,:] = convert_bbox2labels(bbox)\n",
    "    np.savetxt(os.path.join(save_root_dir, \"test.csv\"), testcsv)\n",
    "    print(\"Create %d test data.\" % (n_trainval-n_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (7,7,30) into shape (1470,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/data/study/git/pytorch_cook/05_CV/Yolo/yolov1/data_process.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhg/data/study/git/pytorch_cook/05_CV/Yolo/yolov1/data_process.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     os\u001b[39m.\u001b[39mmkdir(save_dir)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhg/data/study/git/pytorch_cook/05_CV/Yolo/yolov1/data_process.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m anno_dir \u001b[39min\u001b[39;00m anno_dirs:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bhg/data/study/git/pytorch_cook/05_CV/Yolo/yolov1/data_process.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     create_csv_txt(img_dir, anno_dir, save_dir, debug\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;32m/data/study/git/pytorch_cook/05_CV/Yolo/yolov1/data_process.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhg/data/study/git/pytorch_cook/05_CV/Yolo/yolov1/data_process.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(labels_dir,\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.txt\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39mimg_name\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]), \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhg/data/study/git/pytorch_cook/05_CV/Yolo/yolov1/data_process.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m         bbox \u001b[39m=\u001b[39m [\u001b[39mfloat\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m f\u001b[39m.\u001b[39mread()\u001b[39m.\u001b[39msplit()]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bhg/data/study/git/pytorch_cook/05_CV/Yolo/yolov1/data_process.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m         traincsv[i,:] \u001b[39m=\u001b[39m convert_bbox2labels(bbox)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhg/data/study/git/pytorch_cook/05_CV/Yolo/yolov1/data_process.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m np\u001b[39m.\u001b[39msavetxt(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(save_root_dir, \u001b[39m\"\u001b[39m\u001b[39mtrain.csv\u001b[39m\u001b[39m\"\u001b[39m), traincsv)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhg/data/study/git/pytorch_cook/05_CV/Yolo/yolov1/data_process.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCreate \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m train data.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (n_train))\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (7,7,30) into shape (1470,)"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "STATIC_DATASET_PATH = '/data/data/voc/VOCdevkit/VOC2007'\n",
    "img_dir = os.path.join(STATIC_DATASET_PATH, \"JPEGImages\")\n",
    "anno_dirs = [STATIC_DATASET_PATH]\n",
    "save_dir = os.path.join(STATIC_DATASET_PATH, \"voc2012_forYolov1\")\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "for anno_dir in anno_dirs:\n",
    "    create_csv_txt(img_dir, anno_dir, save_dir, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, seed=None, mode=\"train\", train_val_ratio=0.9, trans=None):\n",
    "        if seed is None:\n",
    "            seed = random.randint(0, 65536)\n",
    "        random.seed(seed)\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.mode = mode\n",
    "        if mode == \"val\":\n",
    "            mode = \"train\"\n",
    "\n",
    "        img_list_txt = os.path.join(dataset_dir, mode+\".txt\")\n",
    "        labels_csv = os.path.join(dataset_dir, mode+\".csv\")\n",
    "        self.img_list = []\n",
    "        self.label = np.loadtxt(labels_csv)\n",
    "\n",
    "        with open(img_list_txt, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                self.img_list.append(line.strip())\n",
    "\n",
    "        self.num_all_data = len(self.img_list)\n",
    "        all_ids = list(range(self.num_all_data))\n",
    "        num_train = int(train_val_ratio*self.num_all_data)\n",
    "        if self.mode == \"train\":\n",
    "            self.use_ids = all_ids[:num_train]\n",
    "        elif self.mode == \"val\":\n",
    "            self.use_ids = all_ids[num_train:]\n",
    "        else:\n",
    "            self.use_ids = all_ids\n",
    "\n",
    "        self.trans = trans\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.use_ids)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        id = self.use_ids[item]\n",
    "        label = torch.tensor(self.label[id,:])\n",
    "        img_path = self.img_list[id]\n",
    "        img = Image.open(img_path)\n",
    "        if self.trans is None:\n",
    "            trans = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "            ]) \n",
    "\n",
    "        else:\n",
    "            trans = self.trans\n",
    "\n",
    "        img = trans(img)\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = MyDataset(dataset_dir)\n",
    "dataloader = DataLoader(dataset,1)\n",
    "for i in enumerate(dataloader):\n",
    "    input(\"press enter to coninue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
